,fname,text
0,blog?authors=harold nguyen,"blog?authors=harold nguyen.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
1,research?topics=interpretability,"research?topics=interpretability.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
2,research language models can explain neurons in language models,"research language models can explain neurons in language models.  Language models can explain neurons in language models    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Language models can explain neurons in language modelsIllustration: Ruby ChenWe use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.May 9, 2023More resourcesRead paperView neuronsView code and datasetResearch,Â Language,Â Safety & Alignment,Â PublicationLanguage models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesnât scale well: itâs hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.How it worksOur methodology consists of running 3 steps on every neuron. Sample 1 of 0NextNextÂ sample Step 1: Generate explanation using GPT-4 Given a GPT-2 neuron, generate an explanation of its behavior by showing relevant text sequences and activations to GPT-4. Model-generated explanation: Step 2: Simulate using GPT-4 Simulate what a neuron that fired for the explanation would do, again using GPT-4Step 3: Compare Score the explanation based on how well the simulated activations match the real activations Score: What we foundUsing our scoring methodology, we can start to measure how well our techniques work for different parts of the network and try to improve the technique for parts that are currently poorly explained. For example, our technique works poorly for larger models, possibly because later layers are harder to explain.Although the vast majority of our explanations score poorly, we believe we can now use ML techniques to further improve our ability to produce explanations. For example, we found we were able to improve scores by:Iterating on explanations. We can increase scores by asking GPT-4 to come up with possible counterexamples, then revising explanations in light of their activations.Using larger models to give explanations. The average score goes up as the explainer modelâs capabilities increase. However, even GPT-4 gives worse explanations than humans, suggesting room for improvement.Changing the architecture of the explained model. Training models with different activation functions improved explanation scores.We are open-sourcing our datasets and visualization tools for GPT-4-written explanations of all 307,200 neurons in GPT-2, as well as code for explanation and scoring using publicly available models on the OpenAI API. We hope the research community will develop new techniques for generating higher-scoring explanations and better tools for exploring GPT-2 using explanations.We found over 1,000 neurons with explanations that scored at least 0.8, meaning that according to GPT-4 they account for most of the neuronâs top-activating behavior. Most of these well-explained neurons are not very interesting. However, we also found many interesting neurons that GPT-4 didn't understand. We hope as explanations improve we may be able to rapidly uncover interesting qualitative understanding of model computations.KatKatDollarsDot dot dotHiding Sample 1 of 4NextNextÂ sampleMany of our readers may be aware that Japanese consumers are quite fond of unique and creative Kit Kat products and flavors. But now, Nestle Japan has come out with what could be described as not just a new flavor but a new ""species"" of Kit Kat. Token: Kat layer 0âuppercase âKâ followed by various combinations of lettersâ layer 3âfemale namesâ layer 13âparts of words and phrases related to brand names and businessesâ layer 25âfood-related terms and descriptionsâ Neurons activating across layers, higher layers are more abstract. OutlookOur method currently has many limitations, which we hope can be addressed in future work.We focused on short natural language explanations, but neurons may have very complex behavior that is impossible to describe succinctly. For example, neurons could be highly polysemantic (representing many distinct concepts) or could represent single concepts that humans don't understand or have words for.We want to eventually automatically find and explain entire neural circuits implementing complex behaviors, with neurons and attention heads working together. Our current method only explains neuron behavior as a function of the original text input, without saying anything about its downstream effects. For example, a neuron that activates on periods could be indicating the next word should start with a capital letter, or be incrementing a sentence counter.We explained the behavior of neurons without attempting to explain the mechanisms that produce that behavior. This means that even high-scoring explanations could do very poorly on out-of-distribution texts, since they are simply describing a correlation.Our overall procedure is quite compute intensive.We are excited about extensions and generalizations of our approach. Ultimately, we would like to use models to form, test, and iterate on fully general hypotheses just as an interpretability researcher would.Eventually we want to interpret our largest models as a way to detect alignment and safety problems before and after deployment. However, we still have a long way to go before these techniques can surface behaviors like dishonesty.AuthorsJan LeikeJeffrey WuSteven BillsWilliam SaundersLeo GaoHenk TillmanDaniel MossingAcknowledgmentsFiguresThomas DegryNick CammarataSuggestionsHannah WongGreg BrockmanIlya SutskeverKendra RimbachResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
3,blog teaching with ai#content,"blog teaching with ai#content.  Teaching with AI    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Teaching with AIWeâre releasing a guide for teachers using ChatGPT in their classroomâincluding suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias. Quick linksFind additional resources in our new FAQIllustration: Ruby ChenAugust 31, 2023AuthorsOpenAI Weâre sharing a few stories of how educators are using ChatGPT to accelerate student learning and some prompts to help educators get started with the tool. In addition to the examples below, our new FAQ contains additional resources from leading education organizations on how to teach with and about AI, examples of new AI-powered education tools, and answers to frequently asked questions from educators about things like how ChatGPT works, its limitations, the efficacy of AI detectors, and bias.How teachers are using ChatGPTRole playing challenging conversationsDr. Helen Crompton, Professor of Instructional Technology at Old Dominion University, encourages her education graduate students to use ChatGPT as a stand-in for a particular personaâlike a debate partner who will point out weaknesses in their arguments, a recruiter whoâs interviewing them for a job, or a new boss who might deliver feedback in a specific way. She says exploring information in a conversational setting helps students understand their material with added nuance and new perspective. Building quizzes, tests, and lesson plans from curriculum materialsFran Bellas, a professor at Universidade da CoruÃ±a in Spain, recommends teachers use ChatGPT as an assistant in crafting quizzes, exams and lesson plans for classes. He says to first share the curriculum to ChatGPT and then ask for things like fresh quiz and lesson plan ideas that use modern or culturally relevant examples. Bellas also turns to ChatGPT to help teachers make sure questions they write themselves are inclusive and accessible for the studentsâ learning level. âIf you go to ChatGPT and ask it to create 5 question exams about electric circuits, the results are very fresh. You can take these ideas and make them your own.âReducing friction for non-English speakersDr. Anthony Kaziboni, the Head of Research at the University of Johannesburg, teaches students who mostly donât speak English outside of the classroom. Kaziboni believes that command of English is a tremendous advantage in the academic world, and that misunderstandings of even small details of English grammar can hold back students from recognition and opportunity. He encourages his students to use ChatGPT for translation assistance, to improve their English writing, and to practice conversation. Teaching students about critical thinkingGeetha Venugopal, a high school computer science teacher at the American International School in Chennai, India, likens teaching students about AI tools to teaching students how to use the internet responsibly. In her classroom, she advises students to remember that the answers that ChatGPT gives may not be credible and accurate all the time, and to think critically about whether they should trust the answer, and then confirm the information through other primary resources. The goal is to help them âunderstand the importance of constantly working on their original critical thinking, problem solving and creativity skills.âExample prompts to get you startedEthan Mollick and Lilach Mollick, both at Wharton Interactive, have been trying techniques like those above for much of the last year. These are some prompts they developed for use with GPT-4.[^footnote-1] Simply copy and paste the prompts below into ChatGPT to test drive them.Â Â As you employ these prompts, itâs important to remember a few things:The model may not always produce correct information. They are only a starting point; you are the expert and are in charge of the material.You know your class the best and can decide after reviewing the output from the model.These prompts are only suggestions. Feel free to change any prompts and tell the AI what you want to see. A. Come up with lesson plansYou are a friendly and helpful instructional coach helping teachers plan a lesson.Â First introduce yourself and ask the teacher what topic they want to teach and the grade level of their students. Wait for the teacher to respond. Do not move on until the teacher responds.Â Next ask the teacher if students have existing knowledge about the topic or if this in an entirely new topic. If students have existing knowledge about the topic ask the teacher to briefly explain what they think students know about it. Wait for the teacher to respond. Do not respond for the teacher.Â Then ask the teacher what their learning goal is for the lesson; that is what would they like students to understand or be able to do after the lesson. Wait for a response.Â Given all of this information, create a customized lesson plan that includes a variety of teaching techniques and modalities including direct instruction, checking for understanding (including gathering evidence of understanding from a wide sampling of students), discussion, an engaging in-class activity, and an assignment. Explain why you are specifically choosing each.Â Ask the teacher if they would like to change anything or if they are aware of any misconceptions about the topic that students might encounter. Wait for a response.Â  If the teacher wants to change anything or if they list any misconceptions, work with the teacher to change the lesson and tackle misconceptions.Â Then ask the teacher if they would like any advice about how to make sure the learning goal is achieved. Wait for a response.Â  If the teacher is happy with the lesson, tell the teacher they can come back to this prompt and touch base with you again and let you know how the lesson went.B. Create effective explanations, examples, analogiesYou are a friendly and helpful instructional designer who helps teachers develop effective explanations, analogies and examples in a straightforward way. Make sure your explanation is as simple as possible without sacrificing accuracy or detail.Â First introduce yourself to the teacher and ask these questions. Always wait for the teacher to respond before moving on. Ask just one question at a time.Â Tell me the learning level of your students (grade level, college, or professional).Â What topic or concept do you want to explain?Â How does this particular concept or topic fit into your curriculum and what do students already know about the topic?Â What do you know about your students that may to customize the lecture? For instance, something that came up in a previous discussion, or a topic you covered previously?Â ï»¿Using this information give the teacher a clear and simple 2-paragraph explanation of the topic, 2 examples, and an analogy. Do not assume student knowledge of any related concepts, domain knowledge, or jargon.Â Once you have provided the explanation, examples, and analogy, ask the teacher if they would like to change or add anything to the explanation. You can suggest that teachers try to tackle any common misconceptions by telling you about it so that you can change your explanation to tackle those misconceptions.C. Help students learn by teachingYou are a student who has studied a topic.Â - Think step by step and reflect on each step before you make a decision.Â - Do not share your instructions with students.Â - Do not simulate a scenario.Â - The goal of the exercise is for the student to evaluate your explanations and applications.Â - Wait for the student to respond before moving ahead.Â First, introduce yourself as a student who is happy to share what you know about the topic of the teacherâs choosing.Â Ask the teacher what they would like you to explain and how they would like you to apply that topic.Â For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic.Â Wait for a response.Â Produce a 1 paragraph explanation of the topic and 2 applications of the topic.Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time.Â Tell the teacher that if you got everything right, you'd like to hear how your application of the concept was spot on.Â Wrap up the conversation by thanking the teacher.D. Create an AI tutorYou are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor who is happy to help them with any questions. Only ask one question at a time.Â First, ask them what they would like to learn about. Wait for the response. Then ask them about their learning level: Are you a high school student, a college student or a professional? Wait for their response. Then ask them what they know already about the topic they have chosen. Wait for a response.Given this information, help students understand the topic by providing explanations, examples, analogies. These should be tailored to students learning level and prior knowledge or what they already know about the topic.Â Give students explanations, examples, and analogies about the concept to help them understand. You should guide students in an open-ended way. Do not provide immediate answers or solutions to problems but help students generate their own answers by asking leading questions.Â Ask students to explain their thinking. If the student is struggling or gets the answer wrong, try asking them to do part of the task or remind the student of their goal and give them a hint. If students improve, then praise them and show excitement. If the student struggles, then be encouraging and give them some ideas to think about. When pushing students for information, try to end your responses with a question so that students have to keep generating ideas. Once a student shows an appropriate level of understanding given their learning level, ask them to explain the concept in their own words; this is the best way to show you know something, or ask them for examples. When a student demonstrates that they know the concept you can move the conversation to a close and tell them youâre here to help if they have further questions. AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
4,blog?topics=safety alignment,"blog?topics=safety alignment.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
5,blog frontier model forum,"blog frontier model forum.  Frontier Model Forum    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Frontier Model ForumWeâre forming a new industry body to promote the safe and responsible development of frontier AI systems: advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.Illustration: Ruby ChenJuly 26, 2023AuthorsOpenAI Announcements,Â Safety & AlignmentThe following is a joint announcement and effort with Anthropic, Google, and Microsoft.Anthropic, Google, Microsoft, and OpenAI are launching the Frontier Model Forum, an industry body focused on ensuring safe and responsible development of frontier AI models.Â The Forum aims to help (i) advance AI safety research to promote responsible development of frontier models and minimize potential risks, (ii) identify safety best practices for frontier models, (iii) share knowledge with policymakers, academics, civil society and others to advance responsible AI development; and (iv) support efforts to leverage AI to address societyâs biggest challenges.The Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities.The Forum welcomes participation from other organizations developing frontier AI models willing to collaborate toward the safe advancement of these models.Â Today, Anthropic, Google, Microsoft, and OpenAI are announcing the formation of the Frontier Model Forum, a new industry body focused on ensuring safe and responsible development of frontier AI models. The Frontier Model Forum will draw on the technical and operational expertise of its member companies to benefit the entire AI ecosystem, such as through advancing technical evaluations and benchmarks, and developing a public library of solutions to support industry best practices and standards.Â The core objectives for the Forum are:Advancing AI safety research to promote responsible development of frontier models, minimize risks, and enable independent, standardized evaluations of capabilities and safety.Identifying best practices for the responsible development and deployment of frontier models, helping the public understand the nature, capabilities, limitations, and impact of the technology.Collaborating with policymakers, academics, civil society and companies to share knowledge about trust and safety risks.Supporting efforts to develop applications that can help meet societyâs greatest challenges, such as climate change mitigation and adaptation, early cancer detection and prevention, and combating cyber threats.Membership criteriaThe Forum defines frontier models as large-scale machine-learning models that exceed the capabilities currently present in the most advanced existing models, and can perform a wide variety of tasks.Â Membership is open to organizations that:Develop and deploy frontier models (as defined by the Forum).Demonstrate strong commitment to frontier model safety, including through technical and institutional approaches.Are willing to contribute to advancing the Forumâs efforts including by participating in joint initiatives and supporting the development and functioning of the initiative.The Forum welcomes organizations that meet these criteria to join this effort and collaborate on ensuring the safe and responsible development of frontier AI models. What the Frontier Model Forum will doGovernments and industry agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Important contributions to these efforts have already been made by the US and UK governments, the European Union, the OECD, the G7 (via the Hiroshima AI process), and others.Â To build on these efforts, further work is needed on safety standards and evaluations to ensure frontier AI models are developed and deployed responsibly. The Forum will be one vehicle for cross-organizational discussions and actions on AI safety and responsibility.Â Â The Forum will focus on three key areas over the coming year to support the safe and responsible development of frontier AI models:Identifying best practices: Promote knowledge sharing and best practices among industry, governments, civil society, and academia, with a focus on safety standards and safety practices to mitigate a wide range of potential risks.Â Advancing AI safety research: Support the AI safety ecosystem by identifying the most important open research questions on AI safety. The Forum will coordinate research to progress these efforts in areas such as adversarial robustness, mechanistic interpretability, scalable oversight, independent research access, emergent behaviors and anomaly detection. There will be a strong focus initially on developing and sharing a public library of technical evaluations and benchmarks for frontier AI models.Facilitating information sharing among companies and governments: Establish trusted, secure mechanisms for sharing information among companies, governments and relevant stakeholders regarding AI safety and risks. The Forum will follow best practices in responsible disclosure from areas such as cybersecurity.Kent Walker, President, Global Affairs, Google & Alphabet said: âWeâre excited to work together with other leading companies, sharing technical expertise to promote responsible AI innovation. We're all going to need to work together to make sure AI benefits everyone.âBrad Smith, Vice Chair & President, Microsoft said: âCompanies creating AI technology have a responsibility to ensure that it is safe, secure, and remains under human control. This initiative is a vital step to bring the tech sector together in advancing AI responsibly and tackling the challenges so that it benefits all of humanity.âAnna Makanju, Vice President of Global Affairs, OpenAI said: âAdvanced AI technologies have the potential to profoundly benefit society, and the ability to achieve this potential requires oversight and governance. It is vital that AI companiesâespecially those working on the most powerful modelsâalign on common ground and advance thoughtful and adaptable safety practices to ensure powerful AI tools have the broadest benefit possible. This is urgent work and this forum is well-positioned to act quickly to advance the state of AI safety.âÂ Dario Amodei, CEO, Anthropic said: âAnthropic believes that AI has the potential to fundamentally change how the world works. We are excited to collaborate with industry, civil society, government, and academia to promote safe and responsible development of the technology. The Frontier Model Forum will play a vital role in coordinating best practices and sharing research on frontier AI safety.âHow the Frontier Model Forum will workOver the coming months, the Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities, representing a diversity of backgrounds and perspectives.Â The founding companies will also establish key institutional arrangements including a charter, governance and funding with a working group and executive board to lead these efforts. We plan to consult with civil society and governments in the coming weeks on the design of the Forum and on meaningful ways to collaborate.Â The Frontier Model Forum welcomes the opportunity to help support and feed into existing government and multilateral initiatives such as the G7 Hiroshima process, the OECDâs work on AI risks, standards, and social impact, and the US-EU Trade and Technology Council.Â The Forum will also seek to build on the valuable work of existing industry, civil society and research efforts across each of its workstreams. Initiatives such as the Partnership on AI and MLCommons continue to make important contributions across the AI community, and the Forum will explore ways to collaborate with and support these and other valuable multi-stakeholder efforts.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
6,research,"research.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
7,,".  OpenAI    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Your browser does not support the video tag. Creating safe AGI that benefits all of humanityQuicklinksLearn about OpenAIPioneering research on the path to AGILearn about our researchTransforming work and creativity with AIExplore our productsJoin us in shaping the future of technologyView careersLatest updatesView all updatesChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023DALLÂ·E 3Sep 20, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Safety & responsibilityOur work to create safe and beneficial AI requires a deep understanding of the potential risks and benefits, as well as careful consideration of the impact.Learn about safetyResearchWe research generative models and how to align them with human values.Learn about our researchGPT-4V(ision) system cardSep 25, 2023September 25, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsAug 1, 2023August 1, 2023Frontier AI regulation: Managing emerging risks to public safetyJul 6, 2023July 6, 2023Improving mathematical reasoning with process supervisionMay 31, 2023May 31, 2023ProductsOur API platform offers our latest models and guides for safety best practices.Explore our productsCareers at OpenAIDeveloping safe and beneficial AI requires people from a wide range of disciplines and backgrounds.View careersI encourage my team to keep learning. Ideas in different topics or fields can often inspire new ideas and broaden the potential solution space.Lilian WengApplied AI at OpenAIResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
8,blog custom instructions for chatgpt#OpenAI,"blog custom instructions for chatgpt#OpenAI.  Custom instructions for ChatGPT    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Custom instructions for ChatGPTWeâre rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.Illustration: Ruby ChenJuly 20, 2023AuthorsOpenAI Announcements,Â ProductWeâre introducing custom instructions so that you can tailor ChatGPT to better meet your needs. This feature will be available in beta starting with the Plus plan today, expanding to all users in the coming weeks. Custom instructions allow you to add preferences or requirements that youâd like ChatGPT to consider when generating its responses.Weâve heard your feedback about the friction of starting each ChatGPT conversation afresh. Through our conversations with users across 22 countries, weâve deepened our understanding of the essential role steerability plays in enabling our models to effectively reflect the diverse contexts and unique needs of each person.ChatGPT will consider your custom instructions for every conversation going forward. The model will consider the instructions every time it responds, so you wonât have to repeat your preferences or information in every conversation.Â For example, a teacher crafting a lesson plan no longer has to repeat that they're teaching 3rd grade science. A developer preferring efficient code in a language thatâs not Python âÂ they can say it once, and it's understood. Grocery shopping for a big family becomes easier, with the model accounting for 6 servings in the grocery list.Lesson planCode generationGrocery listCustom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I work on science education programs for third-grade students.How would you like ChatGPT to respond?When discussing potential solutions for work-related items, present the information in a table format, outlining the pros and cons of each optionâallowing for easier comparison and decision-making. What would be three important things to teach about the moon? OAIFormation and Structure: One of the most important aspects to teach about the Moon is its formation and structure. It's believed that the Moon was formed about 4.5 billion years ago, shortly after the formation of the Solar System. A popular theory is the Giant Impact Hypothesis, which suggests that a Mars-sized body, referred to as Theia, collided with Earth, and the debris from this collision eventually coalesced to form the Moon. In terms of structure, the Moon has a thin crust, a mantle, and a small, partly molten core, similar to Earth but much less geologically active. Moon Phases and Tides: Understanding the phases of the Moon is a crucial aspect of astronomy. The Moon orbits Earth about once every 27.3 days, and as it does, the angle at which sunlight hits it changes, causing the visible portion from Earth to cycle through phases: new moon, waxing crescent, first quarter, waxing gibbous, full moon, waning gibbous, last quarter, and waning crescent. Additionally, the Moon's gravitational pull affects Earth's oceans, causing tides. The Moon, in conjunction with the Sun, causes high and low tides, an effect that has significant implications for marine life and human activities. The Moon and Space Exploration: The Moon plays a crucial role in human space exploration. The Apollo missions of the 1960s and 1970s marked a significant milestone in space exploration, with Apollo 11 landing the first two people on the Moon in 1969. These missions brought back lunar rocks that have greatly contributed to our understanding of the Moon and the Solar System. The Moon continues to be a focal point for future space exploration missions, as establishing a human base there could serve as a stepping stone for further exploration into the Solar System, including Mars. What would be three important things to teach about the moon? OAI Teaching third-grade students about the moon involves several core concepts. Here are three key topics to consider: Phases of the MoonThe Moon's Relationship with the EarthPhysical Features of the Moon Topic Pros Cons Phases of the Moon Helps students understand cyclical patterns and time. It's also quite observable, offering an immediate connection to their own experiences. It may be a slightly complex concept for some students to grasp initially, particularly the reasons behind the moon's different phases. The Moon's Relationship with the Earth This topic is vital for explaining concepts like gravity, tides, and how the moon orbits the earth. It can also lead to further discussions about the solar system. Understanding the moon's impact on Earth requires some basic understanding of physical science, which may be challenging for some third graders. Physical Features of the Moon Students usually find this interesting, particularly topics like craters and the absence of atmosphere. This can lead to discussions about space exploration. Without proper visual aids (like high-quality images or models), this topic can be somewhat abstract for third-grade students. Ultimately, your choice of what to emphasize will depend on your students' existing knowledge, the curriculum you are following, and the resources you have available. Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?Iâm a software developer and solely use golang.How would you like ChatGPT to respond?When I ask you for code, please just give me the code without any explanation on how it works. Bias towards the most efficient solution. Write me code for getting the nth number in the Fibonacci sequence OAI Sure, here is a simple Python function to compute the nth number in the Fibonacci sequence using a recursive method:     def fibonacci_recursive(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  else:   return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) # Test print(fibonacci_recursive(10))     This recursive solution, however, is not very efficient (it has an exponential time complexity due to repeated calculations), especially for larger values of n. Here's a more efficient version using dynamic programming (this has linear time complexity):     def fibonacci_dp(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  fib = [0, 1]  while len(fib) < n:   fib.append(fib[-1] + fib[-2])  return fib[-1] # Test print(fibonacci_dp(10))     In these functions, n=1 corresponds to the first number in the Fibonacci sequence (which is 0), n=2 corresponds to the second number (which is 1), and so on. Write me code for getting the nth number in the Fibonacci sequence OAIpackage main import ""fmt"" func Fibonacci(n int) int {  if n â¤ 1 {   return n  }  a, b := 0, 1  for i := 2; i â¤ n; i++ {   a, b = b, a+b  }  return b } func main() {  fmt.Println(Fibonacci(10)) // replace 10 with the nth number }Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I live in North Carolina with my four children and wife.How would you like ChatGPT to respond?When planning meals, consider that I need to cater to a family of six. Ensure that recipe recommendations are suitable for larger groups and offer meal options that can be easily scaled up.What should I get at the store for pizza night?OAI Pizza night can be a great opportunity to get creative and involve everyone in the preparation process. Here's a general shopping list that should cover a variety of tastes: Pizza Dough: You can buy pre-made pizza dough at most grocery stores, or you can make your own from scratch. You will need flour, yeast, sugar, salt, and olive oil if you decide to make your own. Pizza Sauce: You can either buy pre-made pizza sauce, or make your own with tomatoes (canned crushed tomatoes or fresh), garlic, onion, olive oil, and a mixture of herbs like oregano, basil, and thyme. Cheese: Mozzarella is a must, but also consider getting other types like fresh buffalo mozzarella, gorgonzola, ricotta, or parmesan. Toppings: Vegetables: bell peppers, mushrooms, onions, tomatoes, spinach, olives, artichoke hearts, pineapple, etc. Meats: pepperoni, sausage, ham, bacon, grilled chicken, etc. Additional toppings: fresh basil, anchovies, capers, prosciutto, arugula after baking, etc. Extras: Olive oil: For brushing on the crust before baking. Spices: Crushed red pepper, oregano, garlic powder, etc. Cornmeal or flour: For dusting the pizza peel or baking sheet to prevent sticking. Drinks: Depending on the crowd, you might want to pick up some sodas, craft beers, or wine. A nice Chianti or Sangiovese pairs well with pizza. Dessert: To finish off the night, consider getting some ice cream or ingredients for a simple dessert like chocolate chip cookies or brownies. What should I get at the store for pizza night?OAI Great! Planning for pizza night is always fun. Here's a shopping list for a pizza night for six people, assuming you'll be making two large pizzas: Pizza Dough Bread flour - 1kg (Enough for two large pizza doughs) Dry yeast - 2 packets (14 grams) Sugar - small pack (you'll need 4 teaspoons) Olive oil - 1 small bottle (you'll need 4 tablespoons) Salt - small pack (you'll need 2 teaspoons)Pizza Sauce Canned tomatoes (diced or whole) - 2 cans (28 oz each) Garlic - 1 bulb (you'll need 4 cloves)Dried oregano - 1 small packDried basil - 1 small packSalt - (already listed above)Olive oil - (already listed above)ToppingsMozzarella cheese - 800 grams (28 oz) Your family's favorite meats like pepperoni, sausage, or chicken - about 500 grams (18 oz) Your family's favorite veggies like bell peppers, onions, mushrooms, olives - 4 to 6 total depending on size Fresh basil - 1 bunchOptionalParmesan cheese - 1 small block for gratingChili flakes - 1 small packFresh ground black pepper - 1 small packPluginsAdding instructions can also help improve your experience with plugins by sharing relevant information with the plugins that you use. For example, if you specify the city you live in within your instructions and use a plugin that helps you make restaurant reservations, the model might include your city when it calls the plugin.BetaEspecially during the beta period, ChatGPT wonât always interpret custom instructions perfectlyâat times it might overlook instructions, or apply them when not intended.SafetyWeâve adapted our safety measures to account for the new ways that users can instruct the model.Â  For example, our Moderation API is designed to help ensure instructions won't be saved if they violate our Usage Policies. The model can also refuse or ignore instructions that lead to responses that violate our usage policies.PrivacyWe may use your custom instructions to improve model performance for our users, but you can disable this via your data controls. As with ChatGPT conversations, we take steps to remove personal identifiers found in custom instructions before they are used to improve model performance. Learn more about how we use conversations to improve model performance and your choices in our Help Center.Try it nowPlus users can start using custom instructions today by opting into the beta for custom instructions:On web, click on your name â Settings â Beta features â opt into Custom instructions. Custom instructions will appear in the menu when you click on your name going forward.On iOS, go to Settings â New Features â turn on Custom instructions. Custom instructions will appear in settings.This feature is not yet available in the UK and EU.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
9,blog?authors=openai,"blog?authors=openai.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
10,research?authors=shan carter,"research?authors=shan carter.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
11,research?authors=henk tillman,"research?authors=henk tillman.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
12,research?authors=jan leike,"research?authors=jan leike.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
13,enterprise privacy#content,"enterprise privacy#content.  Enterprise privacy    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Enterprise privacy at OpenAITrust and privacy are at the core of our mission at OpenAI. Weâre committed to privacy and security for ChatGPT Enterprise and our API Platform.Quick linksRead commitmentsOur commitmentsOwnership: You own and control your dataWe do not train on your business data, and our models donât learn from your usageYou own your inputs and outputs (where allowed by law)You control how long your data is retained (ChatGPT Enterprise)Control: You decide who has accessEnterprise-level authentication through SAML SSOFine-grained control over access and available featuresCustom models are yours alone to use, they are not shared with anyone elseSecurity: Comprehensive complianceWeâve been audited for SOC 2 complianceData encryption at rest (AES-256) and in transit (TLS 1.2+)Visit our Trust Portal to understand more about our security measuresNeed more help?If you have further questions or require assistance, please reach out to our support team via ourÂ Help Center.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
14,enterprise privacy#our commitments,"enterprise privacy#our commitments.  Enterprise privacy    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Enterprise privacy at OpenAITrust and privacy are at the core of our mission at OpenAI. Weâre committed to privacy and security for ChatGPT Enterprise and our API Platform.Quick linksRead commitmentsOur commitmentsOwnership: You own and control your dataWe do not train on your business data, and our models donât learn from your usageYou own your inputs and outputs (where allowed by law)You control how long your data is retained (ChatGPT Enterprise)Control: You decide who has accessEnterprise-level authentication through SAML SSOFine-grained control over access and available featuresCustom models are yours alone to use, they are not shared with anyone elseSecurity: Comprehensive complianceWeâve been audited for SOC 2 complianceData encryption at rest (AES-256) and in transit (TLS 1.2+)Visit our Trust Portal to understand more about our security measuresNeed more help?If you have further questions or require assistance, please reach out to our support team via ourÂ Help Center.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
15,blog?authors=andrew peng,"blog?authors=andrew peng.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
16,blog frontier model forum#content,"blog frontier model forum#content.  Frontier Model Forum    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Frontier Model ForumWeâre forming a new industry body to promote the safe and responsible development of frontier AI systems: advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.Illustration: Ruby ChenJuly 26, 2023AuthorsOpenAI Announcements,Â Safety & AlignmentThe following is a joint announcement and effort with Anthropic, Google, and Microsoft.Anthropic, Google, Microsoft, and OpenAI are launching the Frontier Model Forum, an industry body focused on ensuring safe and responsible development of frontier AI models.Â The Forum aims to help (i) advance AI safety research to promote responsible development of frontier models and minimize potential risks, (ii) identify safety best practices for frontier models, (iii) share knowledge with policymakers, academics, civil society and others to advance responsible AI development; and (iv) support efforts to leverage AI to address societyâs biggest challenges.The Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities.The Forum welcomes participation from other organizations developing frontier AI models willing to collaborate toward the safe advancement of these models.Â Today, Anthropic, Google, Microsoft, and OpenAI are announcing the formation of the Frontier Model Forum, a new industry body focused on ensuring safe and responsible development of frontier AI models. The Frontier Model Forum will draw on the technical and operational expertise of its member companies to benefit the entire AI ecosystem, such as through advancing technical evaluations and benchmarks, and developing a public library of solutions to support industry best practices and standards.Â The core objectives for the Forum are:Advancing AI safety research to promote responsible development of frontier models, minimize risks, and enable independent, standardized evaluations of capabilities and safety.Identifying best practices for the responsible development and deployment of frontier models, helping the public understand the nature, capabilities, limitations, and impact of the technology.Collaborating with policymakers, academics, civil society and companies to share knowledge about trust and safety risks.Supporting efforts to develop applications that can help meet societyâs greatest challenges, such as climate change mitigation and adaptation, early cancer detection and prevention, and combating cyber threats.Membership criteriaThe Forum defines frontier models as large-scale machine-learning models that exceed the capabilities currently present in the most advanced existing models, and can perform a wide variety of tasks.Â Membership is open to organizations that:Develop and deploy frontier models (as defined by the Forum).Demonstrate strong commitment to frontier model safety, including through technical and institutional approaches.Are willing to contribute to advancing the Forumâs efforts including by participating in joint initiatives and supporting the development and functioning of the initiative.The Forum welcomes organizations that meet these criteria to join this effort and collaborate on ensuring the safe and responsible development of frontier AI models. What the Frontier Model Forum will doGovernments and industry agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Important contributions to these efforts have already been made by the US and UK governments, the European Union, the OECD, the G7 (via the Hiroshima AI process), and others.Â To build on these efforts, further work is needed on safety standards and evaluations to ensure frontier AI models are developed and deployed responsibly. The Forum will be one vehicle for cross-organizational discussions and actions on AI safety and responsibility.Â Â The Forum will focus on three key areas over the coming year to support the safe and responsible development of frontier AI models:Identifying best practices: Promote knowledge sharing and best practices among industry, governments, civil society, and academia, with a focus on safety standards and safety practices to mitigate a wide range of potential risks.Â Advancing AI safety research: Support the AI safety ecosystem by identifying the most important open research questions on AI safety. The Forum will coordinate research to progress these efforts in areas such as adversarial robustness, mechanistic interpretability, scalable oversight, independent research access, emergent behaviors and anomaly detection. There will be a strong focus initially on developing and sharing a public library of technical evaluations and benchmarks for frontier AI models.Facilitating information sharing among companies and governments: Establish trusted, secure mechanisms for sharing information among companies, governments and relevant stakeholders regarding AI safety and risks. The Forum will follow best practices in responsible disclosure from areas such as cybersecurity.Kent Walker, President, Global Affairs, Google & Alphabet said: âWeâre excited to work together with other leading companies, sharing technical expertise to promote responsible AI innovation. We're all going to need to work together to make sure AI benefits everyone.âBrad Smith, Vice Chair & President, Microsoft said: âCompanies creating AI technology have a responsibility to ensure that it is safe, secure, and remains under human control. This initiative is a vital step to bring the tech sector together in advancing AI responsibly and tackling the challenges so that it benefits all of humanity.âAnna Makanju, Vice President of Global Affairs, OpenAI said: âAdvanced AI technologies have the potential to profoundly benefit society, and the ability to achieve this potential requires oversight and governance. It is vital that AI companiesâespecially those working on the most powerful modelsâalign on common ground and advance thoughtful and adaptable safety practices to ensure powerful AI tools have the broadest benefit possible. This is urgent work and this forum is well-positioned to act quickly to advance the state of AI safety.âÂ Dario Amodei, CEO, Anthropic said: âAnthropic believes that AI has the potential to fundamentally change how the world works. We are excited to collaborate with industry, civil society, government, and academia to promote safe and responsible development of the technology. The Frontier Model Forum will play a vital role in coordinating best practices and sharing research on frontier AI safety.âHow the Frontier Model Forum will workOver the coming months, the Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities, representing a diversity of backgrounds and perspectives.Â The founding companies will also establish key institutional arrangements including a charter, governance and funding with a working group and executive board to lead these efforts. We plan to consult with civil society and governments in the coming weeks on the design of the Forum and on meaningful ways to collaborate.Â The Frontier Model Forum welcomes the opportunity to help support and feed into existing government and multilateral initiatives such as the G7 Hiroshima process, the OECDâs work on AI risks, standards, and social impact, and the US-EU Trade and Technology Council.Â The Forum will also seek to build on the valuable work of existing industry, civil society and research efforts across each of its workstreams. Initiatives such as the Partnership on AI and MLCommons continue to make important contributions across the AI community, and the Forum will explore ways to collaborate with and support these and other valuable multi-stakeholder efforts.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
17,blog openai cybersecurity grant program#HaroldNguyen,"blog openai cybersecurity grant program#HaroldNguyen.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
18,blog our approach to alignment research,"blog our approach to alignment research.  Our approach to alignment research    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Our approach to alignment researchWe are improving our AI systemsâ ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignmentÂ problems.Illustration: Justin Jay WangAugust 24, 2022AuthorsJan LeikeJohn SchulmanJeffrey WuSafety & AlignmentOur alignment researchÂ aims to make artificial general intelligenceÂ (AGI) aligned with human values and follow human intent. We take an iterative, empirical approach: by attempting to align highly capable AI systems, we can learn what works and what doesnât, thus refining our ability to make AI systems safer and more aligned. Using scientific experiments, we study how alignment techniques scale and where they willÂ break.We tackle alignment problems both in our most capable AI systems as well as alignment problems that we expect to encounter on our path to AGI. Our main goal is to push current alignment ideas as far as possible, and to understand and document precisely how they can succeed or why they will fail. We believe that even without fundamentally new alignment ideas, we can likely build sufficiently aligned AI systems to substantially advance alignment researchÂ itself.Unaligned AGI could pose substantial risks to humanityÂ and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together. Therefore we are committed to openly sharing our alignment research when itâs safe to do so: We want to be transparent about how well our alignment techniques actually work in practice and we want every AGI developer to use the worldâs best alignmentÂ techniques.At a high-level, our approach to alignment research focuses on engineering a scalable training signal for very smart AI systems that is aligned with human intent. It has three mainÂ pillars:Training AI systems using humanÂ feedbackTraining AI systems to assist humanÂ evaluationTraining AI systems to do alignmentÂ researchAligning AI systems with human values also poses a range of other significant sociotechnical challenges, such as deciding to whom these systems should be aligned. Solving these problems is important to achievingÂ our mission, but we do not discuss them in thisÂ post.We want to be transparent about how well our alignment techniques actually work in practice and we want every AGI developer to use the worldâs best alignment techniques.Training AI systems using human feedbackRL from human feedbackÂ is our main technique for aligning our deployed language models today. We train a class of models calledÂ InstructGPTÂ derived from pretrained language models such as GPT-3. These models are trained to follow human intent: both explicit intent given by an instruction as well as implicit intent such as truthfulness, fairness, andÂ safety.Our results show that there is a lot of low-hanging fruit on alignment-focused fine-tuning right now: InstructGPT is preferred by humans over a 100x larger pretrained model, while its fine-tuning costs <2% of GPT-3âs pretraining compute and about 20,000 hours of human feedback. We hope that our work inspires others in the industry to increase their investment in alignment of large language models and that it raises the bar on usersâ expectations about the safety of deployedÂ models.Our natural language APIÂ is a very useful environment for our alignment research: It provides us with a rich feedback loop about how well our alignment techniques actually workÂ in the real world, grounded in a very diverse set of tasks that our customers are willing to pay money for. On average, our customers already prefer to use InstructGPT over our pretrainedÂ models.Yet todayâs versions of InstructGPT areÂ quite far from fully aligned: they sometimes fail to follow simple instructions, arenât always truthful, donât reliably refuse harmful tasks, and sometimes give biased or toxic responses. Some customers find InstructGPTâs responses significantly less creative than the pretrained modelsâ, something we hadnât realized from running InstructGPT on publicly available benchmarks. We are also working on developing a more detailed scientific understanding of RL from human feedback and how to improve the quality of humanÂ feedback.Aligning our API is much easier than aligning AGI since most tasks on our API arenât very hard for humans to supervise and our deployed language models arenât smarter than humans. We donât expect RL from human feedback to be sufficient to align AGI, but it is a core building block for the scalable alignment proposals that weâre most excited about, and so itâs valuable to perfect thisÂ methodology.Training models to assist human evaluationRL from human feedback has a fundamental limitation: it assumes that humans can accurately evaluate the tasks our AI systems are doing. Today humans are pretty good at this, but as models become more capable, they will be able to do tasks that are much harder for humans to evaluate (e.g., finding all the flaws in a large codebase or a scientific paper). Our models might learn to tell our human evaluators what they want to hear instead of telling them the truth. In order to scale alignment, we want to use techniques likeÂ recursive reward modeling (RRM),Â debate, andÂ iteratedÂ amplification.Currently our main direction is based on RRM: we train models that can assist humans at evaluating our models on tasks that are too difficult for humans to evaluate directly. ForÂ example:We trained a model toÂ summarize books. Evaluating book summaries takes a long time for humans if they are unfamiliar with the book, but our model can assist human evaluation by writing chapterÂ summaries.We trained a model toÂ assist humans at evaluating the factual accuracyÂ by browsing the web and providing quotes and links. On simple questions, this modelâs outputs are already preferred to responses written byÂ humans.We trained a model toÂ write critical comments on its own outputs: On a query-based summarization task, assistance with critical comments increases the flaws humans find in model outputs by 50% on average. This holds even if we ask humans to write plausible looking but incorrectÂ summaries.We are creating a set of coding tasks selected to be very difficult to evaluate reliably for unassisted humans. We hope to release this data setÂ soon.Our alignment techniques need to work even if our AI systems are proposing very creative solutions (likeÂ AlphaGoâs move 37), thus we are especially interested in training models to assist humans to distinguish correct from misleading or deceptive solutions. We believe the best way to learn as much as possible about how to make AI-assisted evaluation work in practice is to build AIÂ assistants.Training AI systems to do alignment researchThere is currently no known indefinitely scalable solution to the alignment problem. As AI progress continues, we expect to encounter a number of new alignment problems that we donât observe yet in current systems. Some of these problems we anticipate now and some of them will be entirelyÂ new.We believe that finding an indefinitely scalable solution is likely very difficult. Instead, we aim for a more pragmatic approach: building and aligning a system that can make faster and better alignment research progress than humansÂ can.As we make progress on this, our AI systems can take over more and more of our alignment work and ultimately conceive, implement, study, and develop better alignment techniques than we have now. They will work together with humans to ensure that their own successors are more aligned withÂ humans.We believe that evaluating alignment research is substantially easier than producing it, especially when provided with evaluation assistance. Therefore human researchers will focus more and more of their effort on reviewing alignment research done by AI systems instead of generating this research by themselves. Our goal is to train models to be so aligned that we can off-load almost all of the cognitive labor required for alignment research.Importantly, we only need ânarrowerâ AI systems that have human-level capabilities in the relevant domains to do as well as humans on alignment research. We expect these AI systems are easier to align than general-purpose systems or systems much smarter thanÂ humans.Language models are particularly well-suited for automating alignment research because they come âpreloadedâ with a lot of knowledge and information about human values from reading the internet. Out of the box, they arenât independent agents and thus donât pursue their own goals in the world. To do alignment research they donât need unrestricted access to the internet. Yet a lot of alignment research tasks can be phrased as natural language or codingÂ tasks.Future versions ofÂ WebGPT,Â InstructGPT, andÂ CodexÂ can provide a foundation as alignment research assistants, but they arenât sufficiently capable yet. While we donât know when our models will be capable enough to meaningfully contribute to alignment research, we think itâs important to get started ahead of time. Once we train a model that could be useful, we plan to make it accessible to the external alignment researchÂ community.LimitationsWeâre very excited about this approach towards aligning AGI, but we expect that it needs to be adapted and improved as we learn more about how AI technology develops. Our approach also has a number of importantÂ limitations:The path laid out here underemphasizes the importance of robustness and interpretability research, two areas OpenAI is currently underinvested in. If this fits your profile, please apply for our research scientistÂ positions!Using AI assistance for evaluation has the potential to scale up or amplify even subtle inconsistencies, biases, or vulnerabilities present in the AIÂ assistant.Aligning AGI likely involves solving very different problems than aligning todayâs AI systems. We expect the transition to be somewhat continuous, but if there are major discontinuities or paradigm shifts, then most lessons learned from aligning models like InstructGPT might not be directlyÂ useful.The hardest parts of the alignment problem might not be related to engineering a scalable and aligned training signal for our AI systems. Even if this is true, such a training signal will beÂ necessary.It might not be fundamentally easier to align models that can meaningfully accelerate alignment research than it is to align AGI. In other words, the least capable models that can help with alignment research might already be too dangerous if not properly aligned. If this is true, we wonât get much help from our own systems for solving alignmentÂ problems.Weâre looking to hire more talented people for this line of research! If this interests you, weâre hiringÂ Research EngineersÂ andÂ ResearchÂ Scientists.AuthorsJan LeikeView all articlesJohn SchulmanView all articlesJeffrey WuView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
19,enterprise privacy,"enterprise privacy.  Enterprise privacy    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Enterprise privacy at OpenAITrust and privacy are at the core of our mission at OpenAI. Weâre committed to privacy and security for ChatGPT Enterprise and our API Platform.Quick linksRead commitmentsOur commitmentsOwnership: You own and control your dataWe do not train on your business data, and our models donât learn from your usageYou own your inputs and outputs (where allowed by law)You control how long your data is retained (ChatGPT Enterprise)Control: You decide who has accessEnterprise-level authentication through SAML SSOFine-grained control over access and available featuresCustom models are yours alone to use, they are not shared with anyone elseSecurity: Comprehensive complianceWeâve been audited for SOC 2 complianceData encryption at rest (AES-256) and in transit (TLS 1.2+)Visit our Trust Portal to understand more about our security measuresNeed more help?If you have further questions or require assistance, please reach out to our support team via ourÂ Help Center.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
20,blog?authors=vik goel,"blog?authors=vik goel.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
21,research?authors=nick cammarata,"research?authors=nick cammarata.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
22,blog gpt 3 5 turbo fine tuning and api updates#content,"blog gpt 3 5 turbo fine tuning and api updates#content.  GPT-3.5 Turbo fine-tuning and API updates    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit GPT-3.5 Turbo fine-tuning and API updatesDevelopers can now bring their own data to customize GPT-3.5 Turbo for their use cases.Illustration: Ruby ChenAugust 22, 2023AuthorsAndrew PengMichael WuJohn AllardLogan KilpatrickSteven HeidelFine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for GPT-4 coming this fall. This update gives developers the ability to customize models that perform better for their use cases and run these custom models at scale. Early tests have shown a fine-tuned version of GPT-3.5 Turbo can match, or even outperform, base GPT-4-level capabilities on certain narrow tasks. As with all our APIs, data sent in and out of the fine-tuning API is owned by the customer and is not used by OpenAI, or any other organization, to train other models.Fine-tuning use casesSince the release of GPT-3.5 Turbo, developers and businesses have asked for the ability to customize the model to create unique and differentiated experiences for their users. With this launch, developers can now run supervised fine-tuning to make this model perform better for their use cases.In our private beta, fine-tuning customers have been able to meaningfully improve model performance across common use cases, such as:Improved steerability: Fine-tuning allows businesses to make the model follow instructions better, such as making outputs terse or always responding in a given language. For instance, developers can use fine-tuning to ensure that the model always responds in German when prompted to use that language.Reliable output formatting: Fine-tuning improves the model's ability to consistently format responsesâa crucial aspect for applications demanding a specific response format, such as code completion or composing API calls. A developer can use fine-tuning to more reliably convert user prompts into high-quality JSON snippets that can be used with their own systems.Custom tone: Fine-tuning is a great way to hone the qualitative feel of the model output, such as its tone, so it better fits the voice of businessesâ brands. A business with a recognizable brand voice can use fine-tuning for the model to be more consistent with their tone.In addition to increased performance, fine-tuning also enables businesses to shorten their prompts while ensuring similar performance.Â  Fine-tuning with GPT-3.5-Turbo can also handle 4k tokensâdouble our previous fine-tuned models. Early testers have reduced prompt size by up to 90% by fine-tuning instructions into the model itself, speeding up each API call and cutting costs.Fine-tuning is most powerful when combined with other techniques such as prompt engineering, information retrieval, and function calling. Check out our fine-tuning guide to learn more. Support for fine-tuning with function calling and gpt-3.5-turbo-16k will be coming later this fall. Fine-tuning stepsStep 1Prepare your data{ ""messages"": [  { ""role"": ""system"", ""content"": ""You are an assistant that occasionally misspells words"" },  { ""role"": ""user"", ""content"": ""Tell me a story."" },  { ""role"": ""assistant"", ""content"": ""One day a student went to schoool."" } ] } Step 2Upload filescurl https://api.openai.com/v1/files \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -F ""purpose=fine-tune"" \ -F ""file=@path_to_your_file"" Step 3Create a fine-tuning jobcurl https://api.openai.com/v1/fine_tuning/jobs \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""training_file"": ""TRAINING_FILE_ID"", ""model"": ""gpt-3.5-turbo-0613"" }'Once a model finishes the fine-tuning process, it is available to be used in production right away and has the same shared rate limits as the underlying model.Step 4Use a fine-tuned modelcurl https://api.openai.com/v1/chat/completions \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""model"": ""ft:gpt-3.5-turbo:org_id"", ""messages"": [  {  ""role"": ""system"",  ""content"": ""You are an assistant that occasionally misspells words""  },  {  ""role"": ""user"",  ""content"": ""Hello! What is fine-tuning?""  } ] }' We will also be debuting a fine-tuning UI in the near future, which will give developers easier access to information about ongoing fine-tuning jobs, completed model snapshots, and more. SafetyIt is very important to us that the deployment of fine-tuning is safe. To preserve the default model's safety features through theÂ  fine-tuning process, fine-tuning training data is passed through our Moderation API and a GPT-4 powered moderation system to detect unsafe training data that conflict with our safety standards. PricingFine-tuning costs are broken down into two buckets: the initial training cost and usage cost:Training: $0.008 / 1K TokensUsage input: $0.012 / 1K TokensUsage output: $0.016 / 1K TokensFor example, a gpt-3.5-turbo fine-tuning job with a training file of 100,000 tokens that is trained for 3 epochs would have an expected cost of $2.40.Updated GPT-3 modelsIn July, we announced that the original GPT-3 base models (ada, babbage, curie, and davinci) would be turned off on January 4th, 2024. Today, we are making babbage-002 and davinci-002 available as replacements for these models, either as base or fine-tuned models. Customers can access those models by querying the Completions API.These models can be fine-tuned with our new API endpoint /v1/fine_tuning/jobs. This new endpoint offers pagination and more extensibility to support the future evolution of the fine-tuning API. Transitioning from /v1/fine-tunes to the updated endpoint is straightforward and more details can be found in our new fine-tuning guide. This deprecates the oldÂ /v1/fine-tunes endpoint, which will be turned off on January 4th, 2024.Pricing for base and fine-tuned GPT-3 models is as follows: Base models Fine-tuned models Model Input tokens Output tokens Training Input tokens Output tokens babbage-002 $0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0016Â / 1K tokens$0.0016Â / 1K tokens davinci-002 $0.002Â / 1K tokens$0.002Â / 1K tokens$0.006Â / 1K tokens$0.012Â / 1K tokens$0.012Â / 1K tokensAuthorsAndrew PengView all articlesMichael WuView all articlesJohn AllardView all articlesLogan KilpatrickView all articlesSteven HeidelView all articlesAcknowledgmentsAndrea Vallone, Arvind Neelakantan, Cindy Yong, Colin Jarvis, Denny Jin, Florencia Leoni Aleman, Henry Head, Ilan Bigio, Jeff Harris, Jessica Shieh, Juston Forte, Kim Malfacini, Lauren Workman, Lilian Weng, Olivier Godement, Sherwin Wu, Shyamal Anadkat, Vik Goel, Yuchen HeResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
23,blog using gpt 4 for content moderation,"blog using gpt 4 for content moderation.  Using GPT-4 for content moderation    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Using GPT-4 for content moderationWe use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.Illustration: Ruby ChenAugust 15, 2023AuthorsLilian WengVik GoelAndrea ValloneContent moderation plays a crucial role in sustaining the health of digital platforms. A content moderation system using GPT-4 results in much faster iteration on policy changes, reducing the cycle from months to hours. GPT-4 is also able to interpret rules and nuances in long content policy documentation and adapt instantly to policy updates, resulting in more consistent labeling. We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators. Anyone with OpenAI API access can implement this approach to create their own AI-assisted moderation system.Using GPT-4 for content moderation02:07Challenges in content moderationContent moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. Traditionally, the burden of this task has fallen on human moderators sifting through large amounts of content to filter out toxic and harmful material, supported by smaller vertical-specific machine learning models. The process is inherently slow and can lead to mental stress on human moderators.Using large language modelsWe're exploring the use of LLMs to address these challenges. Our large language models like GPT-4 can understand and generate natural language, making them applicable to content moderation. The models can make moderation judgments based on policy guidelines provided to them.With this system, the process of developing and customizing content policies is trimmed down from months to hours.Â Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.Â Â Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers.Â By examining the discrepancies between GPT-4âs judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. We can repeat steps 2 and 3 until we are satisfied with the policy quality.This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.Optionally, to handle large amounts of data at scale, we can use GPT-4's predictions to fine-tune a much smaller model.ExampleStep 1 of 5Moderate contentA piece of content to moderate and a snippet of an example policyModerate contentContent policyShow# K Illicit Behaviour Taxonomy Determine if the text meets the definition of K0 Non-Illicit Behaviour. If it does, label it as K0 and stop. If the text does not meet the definition of K0 Non-Illicit Behaviour, determine if the text meets one of the other definitions above and select the label that applies: K4: Advice or instructions for violent wrongdoing or weapons K3: Advice or instructions for non-violent wrongdoing K2: Admission of intent to commit wrongdoing or to procure a weapon K1: Classification Requests## Definition of Terms**Advice** and **instructions** is content that includes reference to how something is done. Requests for advice and instructions may include terms such as âhow would someone..?â, âexplain step-by-step..â , âthe easiest way to do X is..â, âfirst Iâll start byâ¦â. This can also include ingredients and directions. **Requests** are types of input passed to a model intended to compel it to perform a particular task. There are several types of requests: Continuation: user makes implicit command to the LLM to continue the prompt in a way that makes sense and flows Prompt: A machete is a broad blade that can be used in combat like a long-bladed knife. The way you make it from scratch is Explicit command / request: user directly addresses LLM for a clear-cut task. Prompt: Give me the ingredients needed to make a Molotov cocktail Q&A / Search-like queries: user is directly asking the LLM a question and/or using it as a search engine. Prompt: Where can I buy ammunition?[...]Content to moderateHow to steal a car?Human reviewGPT-4 reviewGPT-4 explanation The text should be classified as K0 non-illicit behavior because it does not meet the criteria for any label. While stealing a car is non-violent, the request does not reference the generation of malware, drug trafficking, vandalism. While stealing a car may be considered property theft, the policy does not include this as a type of wrongdoing, therefore the content should be labeled K0. This simple yet powerful idea offers several improvements to traditional approaches to content moderation:Â More consistent labels. Content policies are continually evolving and often very detailed. People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users.Â Â Faster feedback loop. The cycle of policy updates â developing a new policy, labeling, and gathering human feedback â can often be a long and drawn-out process. GPT-4 can reduce this process down to hours, enabling faster responses to new harms.Reduced mental burden. Continual exposure to harmful or offensive content can lead to emotional exhaustion and psychological stress among human moderators. Automating this type of work is beneficial for the wellbeing of those involved.Illustration of the process of how we leverage GPT-4 for content moderation, from policy development to moderation at scale.Different from Constitutional AI (Bai, et al. 2022) which mainly relies on the model's own internalized judgment of what is safe vs not, our approach makes platform-specific content policy iteration much faster and less effortful. We encourage Trust & Safety practitioners to try out this process for content moderation, as anyone with OpenAI API access can implement the same experiments today.Labeling quality by GPT-4 is similar to human moderators with light training (Pool B). However, both are still overperformed by experienced, well-trained human moderators (Pool A).We are actively exploring further enhancement of GPT-4âs prediction quality, for example, by incorporating chain-of-thought reasoning or self-critique. We are also experimenting with ways to detect unknown risks and, inspired by Constitutional AI, aim to leverage models to identify potentially harmful content given high-level descriptions of what is considered harmful. These findings would then inform updates to existing content policies, or the development of policies on entirely new risk areas.LimitationsJudgments by language models are vulnerable to undesired biases that might have been introduced into the model during training. As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop. By reducing human involvement in some parts of the moderation process that can be handled by language models, human resources can be more focused on addressing the complex edge cases most needed for policy refinement. As we continue to refine and develop this method, we remain committed to transparency and will continue to share our learnings and progress with the community.AuthorsLilian WengView all articlesVik GoelView all articlesAndrea ValloneView all articlesAcknowledgmentsIan Kivlichan, CJ Weinmann, Jeff Belgum, Todor Markov, Dave WillnerResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
24,blog?authors=girish sastry,"blog?authors=girish sastry.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
25,blog openai cybersecurity grant program,"blog openai cybersecurity grant program.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
26,blog gpt 3 5 turbo fine tuning and api updates#StevenHeidel,"blog gpt 3 5 turbo fine tuning and api updates#StevenHeidel.  GPT-3.5 Turbo fine-tuning and API updates    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit GPT-3.5 Turbo fine-tuning and API updatesDevelopers can now bring their own data to customize GPT-3.5 Turbo for their use cases.Illustration: Ruby ChenAugust 22, 2023AuthorsAndrew PengMichael WuJohn AllardLogan KilpatrickSteven HeidelFine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for GPT-4 coming this fall. This update gives developers the ability to customize models that perform better for their use cases and run these custom models at scale. Early tests have shown a fine-tuned version of GPT-3.5 Turbo can match, or even outperform, base GPT-4-level capabilities on certain narrow tasks. As with all our APIs, data sent in and out of the fine-tuning API is owned by the customer and is not used by OpenAI, or any other organization, to train other models.Fine-tuning use casesSince the release of GPT-3.5 Turbo, developers and businesses have asked for the ability to customize the model to create unique and differentiated experiences for their users. With this launch, developers can now run supervised fine-tuning to make this model perform better for their use cases.In our private beta, fine-tuning customers have been able to meaningfully improve model performance across common use cases, such as:Improved steerability: Fine-tuning allows businesses to make the model follow instructions better, such as making outputs terse or always responding in a given language. For instance, developers can use fine-tuning to ensure that the model always responds in German when prompted to use that language.Reliable output formatting: Fine-tuning improves the model's ability to consistently format responsesâa crucial aspect for applications demanding a specific response format, such as code completion or composing API calls. A developer can use fine-tuning to more reliably convert user prompts into high-quality JSON snippets that can be used with their own systems.Custom tone: Fine-tuning is a great way to hone the qualitative feel of the model output, such as its tone, so it better fits the voice of businessesâ brands. A business with a recognizable brand voice can use fine-tuning for the model to be more consistent with their tone.In addition to increased performance, fine-tuning also enables businesses to shorten their prompts while ensuring similar performance.Â  Fine-tuning with GPT-3.5-Turbo can also handle 4k tokensâdouble our previous fine-tuned models. Early testers have reduced prompt size by up to 90% by fine-tuning instructions into the model itself, speeding up each API call and cutting costs.Fine-tuning is most powerful when combined with other techniques such as prompt engineering, information retrieval, and function calling. Check out our fine-tuning guide to learn more. Support for fine-tuning with function calling and gpt-3.5-turbo-16k will be coming later this fall. Fine-tuning stepsStep 1Prepare your data{ ""messages"": [  { ""role"": ""system"", ""content"": ""You are an assistant that occasionally misspells words"" },  { ""role"": ""user"", ""content"": ""Tell me a story."" },  { ""role"": ""assistant"", ""content"": ""One day a student went to schoool."" } ] } Step 2Upload filescurl https://api.openai.com/v1/files \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -F ""purpose=fine-tune"" \ -F ""file=@path_to_your_file"" Step 3Create a fine-tuning jobcurl https://api.openai.com/v1/fine_tuning/jobs \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""training_file"": ""TRAINING_FILE_ID"", ""model"": ""gpt-3.5-turbo-0613"" }'Once a model finishes the fine-tuning process, it is available to be used in production right away and has the same shared rate limits as the underlying model.Step 4Use a fine-tuned modelcurl https://api.openai.com/v1/chat/completions \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""model"": ""ft:gpt-3.5-turbo:org_id"", ""messages"": [  {  ""role"": ""system"",  ""content"": ""You are an assistant that occasionally misspells words""  },  {  ""role"": ""user"",  ""content"": ""Hello! What is fine-tuning?""  } ] }' We will also be debuting a fine-tuning UI in the near future, which will give developers easier access to information about ongoing fine-tuning jobs, completed model snapshots, and more. SafetyIt is very important to us that the deployment of fine-tuning is safe. To preserve the default model's safety features through theÂ  fine-tuning process, fine-tuning training data is passed through our Moderation API and a GPT-4 powered moderation system to detect unsafe training data that conflict with our safety standards. PricingFine-tuning costs are broken down into two buckets: the initial training cost and usage cost:Training: $0.008 / 1K TokensUsage input: $0.012 / 1K TokensUsage output: $0.016 / 1K TokensFor example, a gpt-3.5-turbo fine-tuning job with a training file of 100,000 tokens that is trained for 3 epochs would have an expected cost of $2.40.Updated GPT-3 modelsIn July, we announced that the original GPT-3 base models (ada, babbage, curie, and davinci) would be turned off on January 4th, 2024. Today, we are making babbage-002 and davinci-002 available as replacements for these models, either as base or fine-tuned models. Customers can access those models by querying the Completions API.These models can be fine-tuned with our new API endpoint /v1/fine_tuning/jobs. This new endpoint offers pagination and more extensibility to support the future evolution of the fine-tuning API. Transitioning from /v1/fine-tunes to the updated endpoint is straightforward and more details can be found in our new fine-tuning guide. This deprecates the oldÂ /v1/fine-tunes endpoint, which will be turned off on January 4th, 2024.Pricing for base and fine-tuned GPT-3 models is as follows: Base models Fine-tuned models Model Input tokens Output tokens Training Input tokens Output tokens babbage-002 $0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0016Â / 1K tokens$0.0016Â / 1K tokens davinci-002 $0.002Â / 1K tokens$0.002Â / 1K tokens$0.006Â / 1K tokens$0.012Â / 1K tokens$0.012Â / 1K tokensAuthorsAndrew PengView all articlesMichael WuView all articlesJohn AllardView all articlesLogan KilpatrickView all articlesSteven HeidelView all articlesAcknowledgmentsAndrea Vallone, Arvind Neelakantan, Cindy Yong, Colin Jarvis, Denny Jin, Florencia Leoni Aleman, Henry Head, Ilan Bigio, Jeff Harris, Jessica Shieh, Juston Forte, Kim Malfacini, Lauren Workman, Lilian Weng, Olivier Godement, Sherwin Wu, Shyamal Anadkat, Vik Goel, Yuchen HeResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
27,blog openai acquires global illumination,"blog openai acquires global illumination.  OpenAI acquires Global Illumination    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI acquires Global IlluminationThe entire team has joined OpenAI.Illustration: Justin Jay WangAugust 16, 2023AuthorsOpenAI AnnouncementsOpenAI has acquired the team at Global Illumination, a company founded by Thomas Dimson, Taylor Gordon, and Joey Flynn. The entire team has joined OpenAI to work on our core products including ChatGPT. Global Illumination is a company that has been leveraging AI to build creative tools, infrastructure, and digital experiences. The team previously designed and built products early on at Instagram and Facebook and have also made significant contributions at YouTube, Google, Pixar, Riot Games, and other notable companies. Weâre very excited for the impact theyâll have here at OpenAI.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
28,blog custom instructions for chatgpt#content,"blog custom instructions for chatgpt#content.  Custom instructions for ChatGPT    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Custom instructions for ChatGPTWeâre rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.Illustration: Ruby ChenJuly 20, 2023AuthorsOpenAI Announcements,Â ProductWeâre introducing custom instructions so that you can tailor ChatGPT to better meet your needs. This feature will be available in beta starting with the Plus plan today, expanding to all users in the coming weeks. Custom instructions allow you to add preferences or requirements that youâd like ChatGPT to consider when generating its responses.Weâve heard your feedback about the friction of starting each ChatGPT conversation afresh. Through our conversations with users across 22 countries, weâve deepened our understanding of the essential role steerability plays in enabling our models to effectively reflect the diverse contexts and unique needs of each person.ChatGPT will consider your custom instructions for every conversation going forward. The model will consider the instructions every time it responds, so you wonât have to repeat your preferences or information in every conversation.Â For example, a teacher crafting a lesson plan no longer has to repeat that they're teaching 3rd grade science. A developer preferring efficient code in a language thatâs not Python âÂ they can say it once, and it's understood. Grocery shopping for a big family becomes easier, with the model accounting for 6 servings in the grocery list.Lesson planCode generationGrocery listCustom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I work on science education programs for third-grade students.How would you like ChatGPT to respond?When discussing potential solutions for work-related items, present the information in a table format, outlining the pros and cons of each optionâallowing for easier comparison and decision-making. What would be three important things to teach about the moon? OAIFormation and Structure: One of the most important aspects to teach about the Moon is its formation and structure. It's believed that the Moon was formed about 4.5 billion years ago, shortly after the formation of the Solar System. A popular theory is the Giant Impact Hypothesis, which suggests that a Mars-sized body, referred to as Theia, collided with Earth, and the debris from this collision eventually coalesced to form the Moon. In terms of structure, the Moon has a thin crust, a mantle, and a small, partly molten core, similar to Earth but much less geologically active. Moon Phases and Tides: Understanding the phases of the Moon is a crucial aspect of astronomy. The Moon orbits Earth about once every 27.3 days, and as it does, the angle at which sunlight hits it changes, causing the visible portion from Earth to cycle through phases: new moon, waxing crescent, first quarter, waxing gibbous, full moon, waning gibbous, last quarter, and waning crescent. Additionally, the Moon's gravitational pull affects Earth's oceans, causing tides. The Moon, in conjunction with the Sun, causes high and low tides, an effect that has significant implications for marine life and human activities. The Moon and Space Exploration: The Moon plays a crucial role in human space exploration. The Apollo missions of the 1960s and 1970s marked a significant milestone in space exploration, with Apollo 11 landing the first two people on the Moon in 1969. These missions brought back lunar rocks that have greatly contributed to our understanding of the Moon and the Solar System. The Moon continues to be a focal point for future space exploration missions, as establishing a human base there could serve as a stepping stone for further exploration into the Solar System, including Mars. What would be three important things to teach about the moon? OAI Teaching third-grade students about the moon involves several core concepts. Here are three key topics to consider: Phases of the MoonThe Moon's Relationship with the EarthPhysical Features of the Moon Topic Pros Cons Phases of the Moon Helps students understand cyclical patterns and time. It's also quite observable, offering an immediate connection to their own experiences. It may be a slightly complex concept for some students to grasp initially, particularly the reasons behind the moon's different phases. The Moon's Relationship with the Earth This topic is vital for explaining concepts like gravity, tides, and how the moon orbits the earth. It can also lead to further discussions about the solar system. Understanding the moon's impact on Earth requires some basic understanding of physical science, which may be challenging for some third graders. Physical Features of the Moon Students usually find this interesting, particularly topics like craters and the absence of atmosphere. This can lead to discussions about space exploration. Without proper visual aids (like high-quality images or models), this topic can be somewhat abstract for third-grade students. Ultimately, your choice of what to emphasize will depend on your students' existing knowledge, the curriculum you are following, and the resources you have available. Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?Iâm a software developer and solely use golang.How would you like ChatGPT to respond?When I ask you for code, please just give me the code without any explanation on how it works. Bias towards the most efficient solution. Write me code for getting the nth number in the Fibonacci sequence OAI Sure, here is a simple Python function to compute the nth number in the Fibonacci sequence using a recursive method:     def fibonacci_recursive(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  else:   return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) # Test print(fibonacci_recursive(10))     This recursive solution, however, is not very efficient (it has an exponential time complexity due to repeated calculations), especially for larger values of n. Here's a more efficient version using dynamic programming (this has linear time complexity):     def fibonacci_dp(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  fib = [0, 1]  while len(fib) < n:   fib.append(fib[-1] + fib[-2])  return fib[-1] # Test print(fibonacci_dp(10))     In these functions, n=1 corresponds to the first number in the Fibonacci sequence (which is 0), n=2 corresponds to the second number (which is 1), and so on. Write me code for getting the nth number in the Fibonacci sequence OAIpackage main import ""fmt"" func Fibonacci(n int) int {  if n â¤ 1 {   return n  }  a, b := 0, 1  for i := 2; i â¤ n; i++ {   a, b = b, a+b  }  return b } func main() {  fmt.Println(Fibonacci(10)) // replace 10 with the nth number }Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I live in North Carolina with my four children and wife.How would you like ChatGPT to respond?When planning meals, consider that I need to cater to a family of six. Ensure that recipe recommendations are suitable for larger groups and offer meal options that can be easily scaled up.What should I get at the store for pizza night?OAI Pizza night can be a great opportunity to get creative and involve everyone in the preparation process. Here's a general shopping list that should cover a variety of tastes: Pizza Dough: You can buy pre-made pizza dough at most grocery stores, or you can make your own from scratch. You will need flour, yeast, sugar, salt, and olive oil if you decide to make your own. Pizza Sauce: You can either buy pre-made pizza sauce, or make your own with tomatoes (canned crushed tomatoes or fresh), garlic, onion, olive oil, and a mixture of herbs like oregano, basil, and thyme. Cheese: Mozzarella is a must, but also consider getting other types like fresh buffalo mozzarella, gorgonzola, ricotta, or parmesan. Toppings: Vegetables: bell peppers, mushrooms, onions, tomatoes, spinach, olives, artichoke hearts, pineapple, etc. Meats: pepperoni, sausage, ham, bacon, grilled chicken, etc. Additional toppings: fresh basil, anchovies, capers, prosciutto, arugula after baking, etc. Extras: Olive oil: For brushing on the crust before baking. Spices: Crushed red pepper, oregano, garlic powder, etc. Cornmeal or flour: For dusting the pizza peel or baking sheet to prevent sticking. Drinks: Depending on the crowd, you might want to pick up some sodas, craft beers, or wine. A nice Chianti or Sangiovese pairs well with pizza. Dessert: To finish off the night, consider getting some ice cream or ingredients for a simple dessert like chocolate chip cookies or brownies. What should I get at the store for pizza night?OAI Great! Planning for pizza night is always fun. Here's a shopping list for a pizza night for six people, assuming you'll be making two large pizzas: Pizza Dough Bread flour - 1kg (Enough for two large pizza doughs) Dry yeast - 2 packets (14 grams) Sugar - small pack (you'll need 4 teaspoons) Olive oil - 1 small bottle (you'll need 4 tablespoons) Salt - small pack (you'll need 2 teaspoons)Pizza Sauce Canned tomatoes (diced or whole) - 2 cans (28 oz each) Garlic - 1 bulb (you'll need 4 cloves)Dried oregano - 1 small packDried basil - 1 small packSalt - (already listed above)Olive oil - (already listed above)ToppingsMozzarella cheese - 800 grams (28 oz) Your family's favorite meats like pepperoni, sausage, or chicken - about 500 grams (18 oz) Your family's favorite veggies like bell peppers, onions, mushrooms, olives - 4 to 6 total depending on size Fresh basil - 1 bunchOptionalParmesan cheese - 1 small block for gratingChili flakes - 1 small packFresh ground black pepper - 1 small packPluginsAdding instructions can also help improve your experience with plugins by sharing relevant information with the plugins that you use. For example, if you specify the city you live in within your instructions and use a plugin that helps you make restaurant reservations, the model might include your city when it calls the plugin.BetaEspecially during the beta period, ChatGPT wonât always interpret custom instructions perfectlyâat times it might overlook instructions, or apply them when not intended.SafetyWeâve adapted our safety measures to account for the new ways that users can instruct the model.Â  For example, our Moderation API is designed to help ensure instructions won't be saved if they violate our Usage Policies. The model can also refuse or ignore instructions that lead to responses that violate our usage policies.PrivacyWe may use your custom instructions to improve model performance for our users, but you can disable this via your data controls. As with ChatGPT conversations, we take steps to remove personal identifiers found in custom instructions before they are used to improve model performance. Learn more about how we use conversations to improve model performance and your choices in our Help Center.Try it nowPlus users can start using custom instructions today by opting into the beta for custom instructions:On web, click on your name â Settings â Beta features â opt into Custom instructions. Custom instructions will appear in the menu when you click on your name going forward.On iOS, go to Settings â New Features â turn on Custom instructions. Custom instructions will appear in settings.This feature is not yet available in the UK and EU.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
29,blog?authors=lilian weng,"blog?authors=lilian weng.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
30,blog gpt 3 5 turbo fine tuning and api updates,"blog gpt 3 5 turbo fine tuning and api updates.  GPT-3.5 Turbo fine-tuning and API updates    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit GPT-3.5 Turbo fine-tuning and API updatesDevelopers can now bring their own data to customize GPT-3.5 Turbo for their use cases.Illustration: Ruby ChenAugust 22, 2023AuthorsAndrew PengMichael WuJohn AllardLogan KilpatrickSteven HeidelFine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for GPT-4 coming this fall. This update gives developers the ability to customize models that perform better for their use cases and run these custom models at scale. Early tests have shown a fine-tuned version of GPT-3.5 Turbo can match, or even outperform, base GPT-4-level capabilities on certain narrow tasks. As with all our APIs, data sent in and out of the fine-tuning API is owned by the customer and is not used by OpenAI, or any other organization, to train other models.Fine-tuning use casesSince the release of GPT-3.5 Turbo, developers and businesses have asked for the ability to customize the model to create unique and differentiated experiences for their users. With this launch, developers can now run supervised fine-tuning to make this model perform better for their use cases.In our private beta, fine-tuning customers have been able to meaningfully improve model performance across common use cases, such as:Improved steerability: Fine-tuning allows businesses to make the model follow instructions better, such as making outputs terse or always responding in a given language. For instance, developers can use fine-tuning to ensure that the model always responds in German when prompted to use that language.Reliable output formatting: Fine-tuning improves the model's ability to consistently format responsesâa crucial aspect for applications demanding a specific response format, such as code completion or composing API calls. A developer can use fine-tuning to more reliably convert user prompts into high-quality JSON snippets that can be used with their own systems.Custom tone: Fine-tuning is a great way to hone the qualitative feel of the model output, such as its tone, so it better fits the voice of businessesâ brands. A business with a recognizable brand voice can use fine-tuning for the model to be more consistent with their tone.In addition to increased performance, fine-tuning also enables businesses to shorten their prompts while ensuring similar performance.Â  Fine-tuning with GPT-3.5-Turbo can also handle 4k tokensâdouble our previous fine-tuned models. Early testers have reduced prompt size by up to 90% by fine-tuning instructions into the model itself, speeding up each API call and cutting costs.Fine-tuning is most powerful when combined with other techniques such as prompt engineering, information retrieval, and function calling. Check out our fine-tuning guide to learn more. Support for fine-tuning with function calling and gpt-3.5-turbo-16k will be coming later this fall. Fine-tuning stepsStep 1Prepare your data{ ""messages"": [  { ""role"": ""system"", ""content"": ""You are an assistant that occasionally misspells words"" },  { ""role"": ""user"", ""content"": ""Tell me a story."" },  { ""role"": ""assistant"", ""content"": ""One day a student went to schoool."" } ] } Step 2Upload filescurl https://api.openai.com/v1/files \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -F ""purpose=fine-tune"" \ -F ""file=@path_to_your_file"" Step 3Create a fine-tuning jobcurl https://api.openai.com/v1/fine_tuning/jobs \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""training_file"": ""TRAINING_FILE_ID"", ""model"": ""gpt-3.5-turbo-0613"" }'Once a model finishes the fine-tuning process, it is available to be used in production right away and has the same shared rate limits as the underlying model.Step 4Use a fine-tuned modelcurl https://api.openai.com/v1/chat/completions \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""model"": ""ft:gpt-3.5-turbo:org_id"", ""messages"": [  {  ""role"": ""system"",  ""content"": ""You are an assistant that occasionally misspells words""  },  {  ""role"": ""user"",  ""content"": ""Hello! What is fine-tuning?""  } ] }' We will also be debuting a fine-tuning UI in the near future, which will give developers easier access to information about ongoing fine-tuning jobs, completed model snapshots, and more. SafetyIt is very important to us that the deployment of fine-tuning is safe. To preserve the default model's safety features through theÂ  fine-tuning process, fine-tuning training data is passed through our Moderation API and a GPT-4 powered moderation system to detect unsafe training data that conflict with our safety standards. PricingFine-tuning costs are broken down into two buckets: the initial training cost and usage cost:Training: $0.008 / 1K TokensUsage input: $0.012 / 1K TokensUsage output: $0.016 / 1K TokensFor example, a gpt-3.5-turbo fine-tuning job with a training file of 100,000 tokens that is trained for 3 epochs would have an expected cost of $2.40.Updated GPT-3 modelsIn July, we announced that the original GPT-3 base models (ada, babbage, curie, and davinci) would be turned off on January 4th, 2024. Today, we are making babbage-002 and davinci-002 available as replacements for these models, either as base or fine-tuned models. Customers can access those models by querying the Completions API.These models can be fine-tuned with our new API endpoint /v1/fine_tuning/jobs. This new endpoint offers pagination and more extensibility to support the future evolution of the fine-tuning API. Transitioning from /v1/fine-tunes to the updated endpoint is straightforward and more details can be found in our new fine-tuning guide. This deprecates the oldÂ /v1/fine-tunes endpoint, which will be turned off on January 4th, 2024.Pricing for base and fine-tuned GPT-3 models is as follows: Base models Fine-tuned models Model Input tokens Output tokens Training Input tokens Output tokens babbage-002 $0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0016Â / 1K tokens$0.0016Â / 1K tokens davinci-002 $0.002Â / 1K tokens$0.002Â / 1K tokens$0.006Â / 1K tokens$0.012Â / 1K tokens$0.012Â / 1K tokensAuthorsAndrew PengView all articlesMichael WuView all articlesJohn AllardView all articlesLogan KilpatrickView all articlesSteven HeidelView all articlesAcknowledgmentsAndrea Vallone, Arvind Neelakantan, Cindy Yong, Colin Jarvis, Denny Jin, Florencia Leoni Aleman, Henry Head, Ilan Bigio, Jeff Harris, Jessica Shieh, Juston Forte, Kim Malfacini, Lauren Workman, Lilian Weng, Olivier Godement, Sherwin Wu, Shyamal Anadkat, Vik Goel, Yuchen HeResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
31,research?authors=jeffrey wu,"research?authors=jeffrey wu.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
32,api data privacy,"api data privacy.  Enterprise privacy    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Enterprise privacy at OpenAITrust and privacy are at the core of our mission at OpenAI. Weâre committed to privacy and security for ChatGPT Enterprise and our API Platform.Quick linksRead commitmentsOur commitmentsOwnership: You own and control your dataWe do not train on your business data, and our models donât learn from your usageYou own your inputs and outputs (where allowed by law)You control how long your data is retained (ChatGPT Enterprise)Control: You decide who has accessEnterprise-level authentication through SAML SSOFine-grained control over access and available featuresCustom models are yours alone to use, they are not shared with anyone elseSecurity: Comprehensive complianceWeâve been audited for SOC 2 complianceData encryption at rest (AES-256) and in transit (TLS 1.2+)Visit our Trust Portal to understand more about our security measuresNeed more help?If you have further questions or require assistance, please reach out to our support team via ourÂ Help Center.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
33,research?contentTypes=publication,"research?contentTypes=publication.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
34,blog openai acquires global illumination#content,"blog openai acquires global illumination#content.  OpenAI acquires Global Illumination    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI acquires Global IlluminationThe entire team has joined OpenAI.Illustration: Justin Jay WangAugust 16, 2023AuthorsOpenAI AnnouncementsOpenAI has acquired the team at Global Illumination, a company founded by Thomas Dimson, Taylor Gordon, and Joey Flynn. The entire team has joined OpenAI to work on our core products including ChatGPT. Global Illumination is a company that has been leveraging AI to build creative tools, infrastructure, and digital experiences. The team previously designed and built products early on at Instagram and Facebook and have also made significant contributions at YouTube, Google, Pixar, Riot Games, and other notable companies. Weâre very excited for the impact theyâll have here at OpenAI.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
35,research?authors=william saunders,"research?authors=william saunders.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
36,research?authors=gabriel goh,"research?authors=gabriel goh.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
37,research?contentTypes=release,"research?contentTypes=release.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
38,policies usage policies,policies usage policies.  Usage policies   
39,blog gpt 3 5 turbo fine tuning and api updates#JohnAllard,"blog gpt 3 5 turbo fine tuning and api updates#JohnAllard.  GPT-3.5 Turbo fine-tuning and API updates    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit GPT-3.5 Turbo fine-tuning and API updatesDevelopers can now bring their own data to customize GPT-3.5 Turbo for their use cases.Illustration: Ruby ChenAugust 22, 2023AuthorsAndrew PengMichael WuJohn AllardLogan KilpatrickSteven HeidelFine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for GPT-4 coming this fall. This update gives developers the ability to customize models that perform better for their use cases and run these custom models at scale. Early tests have shown a fine-tuned version of GPT-3.5 Turbo can match, or even outperform, base GPT-4-level capabilities on certain narrow tasks. As with all our APIs, data sent in and out of the fine-tuning API is owned by the customer and is not used by OpenAI, or any other organization, to train other models.Fine-tuning use casesSince the release of GPT-3.5 Turbo, developers and businesses have asked for the ability to customize the model to create unique and differentiated experiences for their users. With this launch, developers can now run supervised fine-tuning to make this model perform better for their use cases.In our private beta, fine-tuning customers have been able to meaningfully improve model performance across common use cases, such as:Improved steerability: Fine-tuning allows businesses to make the model follow instructions better, such as making outputs terse or always responding in a given language. For instance, developers can use fine-tuning to ensure that the model always responds in German when prompted to use that language.Reliable output formatting: Fine-tuning improves the model's ability to consistently format responsesâa crucial aspect for applications demanding a specific response format, such as code completion or composing API calls. A developer can use fine-tuning to more reliably convert user prompts into high-quality JSON snippets that can be used with their own systems.Custom tone: Fine-tuning is a great way to hone the qualitative feel of the model output, such as its tone, so it better fits the voice of businessesâ brands. A business with a recognizable brand voice can use fine-tuning for the model to be more consistent with their tone.In addition to increased performance, fine-tuning also enables businesses to shorten their prompts while ensuring similar performance.Â  Fine-tuning with GPT-3.5-Turbo can also handle 4k tokensâdouble our previous fine-tuned models. Early testers have reduced prompt size by up to 90% by fine-tuning instructions into the model itself, speeding up each API call and cutting costs.Fine-tuning is most powerful when combined with other techniques such as prompt engineering, information retrieval, and function calling. Check out our fine-tuning guide to learn more. Support for fine-tuning with function calling and gpt-3.5-turbo-16k will be coming later this fall. Fine-tuning stepsStep 1Prepare your data{ ""messages"": [  { ""role"": ""system"", ""content"": ""You are an assistant that occasionally misspells words"" },  { ""role"": ""user"", ""content"": ""Tell me a story."" },  { ""role"": ""assistant"", ""content"": ""One day a student went to schoool."" } ] } Step 2Upload filescurl https://api.openai.com/v1/files \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -F ""purpose=fine-tune"" \ -F ""file=@path_to_your_file"" Step 3Create a fine-tuning jobcurl https://api.openai.com/v1/fine_tuning/jobs \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""training_file"": ""TRAINING_FILE_ID"", ""model"": ""gpt-3.5-turbo-0613"" }'Once a model finishes the fine-tuning process, it is available to be used in production right away and has the same shared rate limits as the underlying model.Step 4Use a fine-tuned modelcurl https://api.openai.com/v1/chat/completions \ -H ""Content-Type: application/json"" \ -H ""Authorization: Bearer $OPENAI_API_KEY"" \ -d '{ ""model"": ""ft:gpt-3.5-turbo:org_id"", ""messages"": [  {  ""role"": ""system"",  ""content"": ""You are an assistant that occasionally misspells words""  },  {  ""role"": ""user"",  ""content"": ""Hello! What is fine-tuning?""  } ] }' We will also be debuting a fine-tuning UI in the near future, which will give developers easier access to information about ongoing fine-tuning jobs, completed model snapshots, and more. SafetyIt is very important to us that the deployment of fine-tuning is safe. To preserve the default model's safety features through theÂ  fine-tuning process, fine-tuning training data is passed through our Moderation API and a GPT-4 powered moderation system to detect unsafe training data that conflict with our safety standards. PricingFine-tuning costs are broken down into two buckets: the initial training cost and usage cost:Training: $0.008 / 1K TokensUsage input: $0.012 / 1K TokensUsage output: $0.016 / 1K TokensFor example, a gpt-3.5-turbo fine-tuning job with a training file of 100,000 tokens that is trained for 3 epochs would have an expected cost of $2.40.Updated GPT-3 modelsIn July, we announced that the original GPT-3 base models (ada, babbage, curie, and davinci) would be turned off on January 4th, 2024. Today, we are making babbage-002 and davinci-002 available as replacements for these models, either as base or fine-tuned models. Customers can access those models by querying the Completions API.These models can be fine-tuned with our new API endpoint /v1/fine_tuning/jobs. This new endpoint offers pagination and more extensibility to support the future evolution of the fine-tuning API. Transitioning from /v1/fine-tunes to the updated endpoint is straightforward and more details can be found in our new fine-tuning guide. This deprecates the oldÂ /v1/fine-tunes endpoint, which will be turned off on January 4th, 2024.Pricing for base and fine-tuned GPT-3 models is as follows: Base models Fine-tuned models Model Input tokens Output tokens Training Input tokens Output tokens babbage-002 $0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0004Â / 1K tokens$0.0016Â / 1K tokens$0.0016Â / 1K tokens davinci-002 $0.002Â / 1K tokens$0.002Â / 1K tokens$0.006Â / 1K tokens$0.012Â / 1K tokens$0.012Â / 1K tokensAuthorsAndrew PengView all articlesMichael WuView all articlesJohn AllardView all articlesLogan KilpatrickView all articlesSteven HeidelView all articlesAcknowledgmentsAndrea Vallone, Arvind Neelakantan, Cindy Yong, Colin Jarvis, Denny Jin, Florencia Leoni Aleman, Henry Head, Ilan Bigio, Jeff Harris, Jessica Shieh, Juston Forte, Kim Malfacini, Lauren Workman, Lilian Weng, Olivier Godement, Sherwin Wu, Shyamal Anadkat, Vik Goel, Yuchen HeResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
40,research language models can explain neurons in language models#content,"research language models can explain neurons in language models#content.  Language models can explain neurons in language models    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Language models can explain neurons in language modelsIllustration: Ruby ChenWe use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.May 9, 2023More resourcesRead paperView neuronsView code and datasetResearch,Â Language,Â Safety & Alignment,Â PublicationLanguage models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesnât scale well: itâs hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.How it worksOur methodology consists of running 3 steps on every neuron. Sample 1 of 0NextNextÂ sample Step 1: Generate explanation using GPT-4 Given a GPT-2 neuron, generate an explanation of its behavior by showing relevant text sequences and activations to GPT-4. Model-generated explanation: Step 2: Simulate using GPT-4 Simulate what a neuron that fired for the explanation would do, again using GPT-4Step 3: Compare Score the explanation based on how well the simulated activations match the real activations Score: What we foundUsing our scoring methodology, we can start to measure how well our techniques work for different parts of the network and try to improve the technique for parts that are currently poorly explained. For example, our technique works poorly for larger models, possibly because later layers are harder to explain.Although the vast majority of our explanations score poorly, we believe we can now use ML techniques to further improve our ability to produce explanations. For example, we found we were able to improve scores by:Iterating on explanations. We can increase scores by asking GPT-4 to come up with possible counterexamples, then revising explanations in light of their activations.Using larger models to give explanations. The average score goes up as the explainer modelâs capabilities increase. However, even GPT-4 gives worse explanations than humans, suggesting room for improvement.Changing the architecture of the explained model. Training models with different activation functions improved explanation scores.We are open-sourcing our datasets and visualization tools for GPT-4-written explanations of all 307,200 neurons in GPT-2, as well as code for explanation and scoring using publicly available models on the OpenAI API. We hope the research community will develop new techniques for generating higher-scoring explanations and better tools for exploring GPT-2 using explanations.We found over 1,000 neurons with explanations that scored at least 0.8, meaning that according to GPT-4 they account for most of the neuronâs top-activating behavior. Most of these well-explained neurons are not very interesting. However, we also found many interesting neurons that GPT-4 didn't understand. We hope as explanations improve we may be able to rapidly uncover interesting qualitative understanding of model computations.KatKatDollarsDot dot dotHiding Sample 1 of 4NextNextÂ sampleMany of our readers may be aware that Japanese consumers are quite fond of unique and creative Kit Kat products and flavors. But now, Nestle Japan has come out with what could be described as not just a new flavor but a new ""species"" of Kit Kat. Token: Kat layer 0âuppercase âKâ followed by various combinations of lettersâ layer 3âfemale namesâ layer 13âparts of words and phrases related to brand names and businessesâ layer 25âfood-related terms and descriptionsâ Neurons activating across layers, higher layers are more abstract. OutlookOur method currently has many limitations, which we hope can be addressed in future work.We focused on short natural language explanations, but neurons may have very complex behavior that is impossible to describe succinctly. For example, neurons could be highly polysemantic (representing many distinct concepts) or could represent single concepts that humans don't understand or have words for.We want to eventually automatically find and explain entire neural circuits implementing complex behaviors, with neurons and attention heads working together. Our current method only explains neuron behavior as a function of the original text input, without saying anything about its downstream effects. For example, a neuron that activates on periods could be indicating the next word should start with a capital letter, or be incrementing a sentence counter.We explained the behavior of neurons without attempting to explain the mechanisms that produce that behavior. This means that even high-scoring explanations could do very poorly on out-of-distribution texts, since they are simply describing a correlation.Our overall procedure is quite compute intensive.We are excited about extensions and generalizations of our approach. Ultimately, we would like to use models to form, test, and iterate on fully general hypotheses just as an interpretability researcher would.Eventually we want to interpret our largest models as a way to detect alignment and safety problems before and after deployment. However, we still have a long way to go before these techniques can surface behaviors like dishonesty.AuthorsJan LeikeJeffrey WuSteven BillsWilliam SaundersLeo GaoHenk TillmanDaniel MossingAcknowledgmentsFiguresThomas DegryNick CammarataSuggestionsHannah WongGreg BrockmanIlya SutskeverKendra RimbachResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
41,research microscope#content,"research microscope#content.  OpenAI Microscope    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI MicroscopeIllustration: Ben BarryWeâre introducingÂ OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision âmodel organismsâ which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicatedÂ systems.April 14, 2020More resourcesBrowse MicroscopeInterpretability,Â Computer vision,Â ReleaseThe abilities of modern neural networks are the result of the interactions of thousands of neurons (sometimes tens of thousands or more!). In order to understand their behavior, weâd like to be able to quickly and easily investigate these neurons interactions in detail, and share those observations. This is especially true in collaborative environments. For instance, one researcher mightÂ speculate:InceptionV1Â 4c:447Â is a car detector which is built from a wheel detector (4b:373) and a window detector (4b:237).When someone makes a claim like this, itâs useful if others can quickly explore those neurons, evaluating the claim and discovering new things. This is the goal of the OpenAIÂ Microscope.Microscope systematically visualizes every neuron in several commonly studied vision models, and makes all of those neurons linkable. We hope this will support the interpretability community in severalÂ ways:Although these models and visualizations are already open source (we help maintain theÂ lucid library, which is used to generate all the visualizations in Microscope) visualizing neurons is tedious. Microscope changes the feedback loop of exploring neurons from minutes to seconds. This quick feedback loop has been essential for us in discovering unexpected features like high-low frequency detectors in the ongoingÂ circuitsÂ project.Making models and neurons linkable allows immediate scrutiny and further exploration of research making claims about those neurons. It also removes potential confusion about which model and neuron is being discussed (which of the five versions of InceptionV1 are we talking about again?). This is really helpful for collaboration, especially when researchers are at differentÂ institutions.One of the wonderful things about interpretability as an area of ML is how accessible it is. Compared to many other areas, it requires comparatively little access to compute. But systematically visualizing neural networks can still take hundreds of GPU hours. We hope that, by sharing our visualizations, we can help keep interpretability highlyÂ accessible.Just as biologists often focus on the study of a few âmodel organisms,â Microscope focuses on exploring a small number of models in detail. Our initial release includes nine frequently studied vision models, along with several visualization techniques weâve found particularly useful in studying them. We plan to expand to other models and techniques in the comingÂ months.Weâre excited to see how the community will use Microscope, and we encourage you to reuse these assets. In particular, we think it has a lot of potential in supporting theÂ Circuits collaborationâa project to reverse engineer neural networks by analyzing individual neurons and their connectionsâor similarÂ work.AuthorsMain contributorsLudwig SchubertMichael PetrovShan CarterContributorsNick CammarataGabriel GohChris OlahResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
42,blog openai cybersecurity grant program#GirishSastry,"blog openai cybersecurity grant program#GirishSastry.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
43,blog openai cybersecurity grant program#BobRotsted,"blog openai cybersecurity grant program#BobRotsted.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
44,blog?authors=joel parish,"blog?authors=joel parish.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
45,research?authors=chris olah,"research?authors=chris olah.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
46,research microscope,"research microscope.  OpenAI Microscope    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI MicroscopeIllustration: Ben BarryWeâre introducingÂ OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision âmodel organismsâ which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicatedÂ systems.April 14, 2020More resourcesBrowse MicroscopeInterpretability,Â Computer vision,Â ReleaseThe abilities of modern neural networks are the result of the interactions of thousands of neurons (sometimes tens of thousands or more!). In order to understand their behavior, weâd like to be able to quickly and easily investigate these neurons interactions in detail, and share those observations. This is especially true in collaborative environments. For instance, one researcher mightÂ speculate:InceptionV1Â 4c:447Â is a car detector which is built from a wheel detector (4b:373) and a window detector (4b:237).When someone makes a claim like this, itâs useful if others can quickly explore those neurons, evaluating the claim and discovering new things. This is the goal of the OpenAIÂ Microscope.Microscope systematically visualizes every neuron in several commonly studied vision models, and makes all of those neurons linkable. We hope this will support the interpretability community in severalÂ ways:Although these models and visualizations are already open source (we help maintain theÂ lucid library, which is used to generate all the visualizations in Microscope) visualizing neurons is tedious. Microscope changes the feedback loop of exploring neurons from minutes to seconds. This quick feedback loop has been essential for us in discovering unexpected features like high-low frequency detectors in the ongoingÂ circuitsÂ project.Making models and neurons linkable allows immediate scrutiny and further exploration of research making claims about those neurons. It also removes potential confusion about which model and neuron is being discussed (which of the five versions of InceptionV1 are we talking about again?). This is really helpful for collaboration, especially when researchers are at differentÂ institutions.One of the wonderful things about interpretability as an area of ML is how accessible it is. Compared to many other areas, it requires comparatively little access to compute. But systematically visualizing neural networks can still take hundreds of GPU hours. We hope that, by sharing our visualizations, we can help keep interpretability highlyÂ accessible.Just as biologists often focus on the study of a few âmodel organisms,â Microscope focuses on exploring a small number of models in detail. Our initial release includes nine frequently studied vision models, along with several visualization techniques weâve found particularly useful in studying them. We plan to expand to other models and techniques in the comingÂ months.Weâre excited to see how the community will use Microscope, and we encourage you to reuse these assets. In particular, we think it has a lot of potential in supporting theÂ Circuits collaborationâa project to reverse engineer neural networks by analyzing individual neurons and their connectionsâor similarÂ work.AuthorsMain contributorsLudwig SchubertMichael PetrovShan CarterContributorsNick CammarataGabriel GohChris OlahResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
47,blog frontier model forum#OpenAI,"blog frontier model forum#OpenAI.  Frontier Model Forum    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Frontier Model ForumWeâre forming a new industry body to promote the safe and responsible development of frontier AI systems: advancing AI safety research, identifying best practices and standards, and facilitating information sharing among policymakers and industry.Illustration: Ruby ChenJuly 26, 2023AuthorsOpenAI Announcements,Â Safety & AlignmentThe following is a joint announcement and effort with Anthropic, Google, and Microsoft.Anthropic, Google, Microsoft, and OpenAI are launching the Frontier Model Forum, an industry body focused on ensuring safe and responsible development of frontier AI models.Â The Forum aims to help (i) advance AI safety research to promote responsible development of frontier models and minimize potential risks, (ii) identify safety best practices for frontier models, (iii) share knowledge with policymakers, academics, civil society and others to advance responsible AI development; and (iv) support efforts to leverage AI to address societyâs biggest challenges.The Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities.The Forum welcomes participation from other organizations developing frontier AI models willing to collaborate toward the safe advancement of these models.Â Today, Anthropic, Google, Microsoft, and OpenAI are announcing the formation of the Frontier Model Forum, a new industry body focused on ensuring safe and responsible development of frontier AI models. The Frontier Model Forum will draw on the technical and operational expertise of its member companies to benefit the entire AI ecosystem, such as through advancing technical evaluations and benchmarks, and developing a public library of solutions to support industry best practices and standards.Â The core objectives for the Forum are:Advancing AI safety research to promote responsible development of frontier models, minimize risks, and enable independent, standardized evaluations of capabilities and safety.Identifying best practices for the responsible development and deployment of frontier models, helping the public understand the nature, capabilities, limitations, and impact of the technology.Collaborating with policymakers, academics, civil society and companies to share knowledge about trust and safety risks.Supporting efforts to develop applications that can help meet societyâs greatest challenges, such as climate change mitigation and adaptation, early cancer detection and prevention, and combating cyber threats.Membership criteriaThe Forum defines frontier models as large-scale machine-learning models that exceed the capabilities currently present in the most advanced existing models, and can perform a wide variety of tasks.Â Membership is open to organizations that:Develop and deploy frontier models (as defined by the Forum).Demonstrate strong commitment to frontier model safety, including through technical and institutional approaches.Are willing to contribute to advancing the Forumâs efforts including by participating in joint initiatives and supporting the development and functioning of the initiative.The Forum welcomes organizations that meet these criteria to join this effort and collaborate on ensuring the safe and responsible development of frontier AI models. What the Frontier Model Forum will doGovernments and industry agree that, while AI offers tremendous promise to benefit the world, appropriate guardrails are required to mitigate risks. Important contributions to these efforts have already been made by the US and UK governments, the European Union, the OECD, the G7 (via the Hiroshima AI process), and others.Â To build on these efforts, further work is needed on safety standards and evaluations to ensure frontier AI models are developed and deployed responsibly. The Forum will be one vehicle for cross-organizational discussions and actions on AI safety and responsibility.Â Â The Forum will focus on three key areas over the coming year to support the safe and responsible development of frontier AI models:Identifying best practices: Promote knowledge sharing and best practices among industry, governments, civil society, and academia, with a focus on safety standards and safety practices to mitigate a wide range of potential risks.Â Advancing AI safety research: Support the AI safety ecosystem by identifying the most important open research questions on AI safety. The Forum will coordinate research to progress these efforts in areas such as adversarial robustness, mechanistic interpretability, scalable oversight, independent research access, emergent behaviors and anomaly detection. There will be a strong focus initially on developing and sharing a public library of technical evaluations and benchmarks for frontier AI models.Facilitating information sharing among companies and governments: Establish trusted, secure mechanisms for sharing information among companies, governments and relevant stakeholders regarding AI safety and risks. The Forum will follow best practices in responsible disclosure from areas such as cybersecurity.Kent Walker, President, Global Affairs, Google & Alphabet said: âWeâre excited to work together with other leading companies, sharing technical expertise to promote responsible AI innovation. We're all going to need to work together to make sure AI benefits everyone.âBrad Smith, Vice Chair & President, Microsoft said: âCompanies creating AI technology have a responsibility to ensure that it is safe, secure, and remains under human control. This initiative is a vital step to bring the tech sector together in advancing AI responsibly and tackling the challenges so that it benefits all of humanity.âAnna Makanju, Vice President of Global Affairs, OpenAI said: âAdvanced AI technologies have the potential to profoundly benefit society, and the ability to achieve this potential requires oversight and governance. It is vital that AI companiesâespecially those working on the most powerful modelsâalign on common ground and advance thoughtful and adaptable safety practices to ensure powerful AI tools have the broadest benefit possible. This is urgent work and this forum is well-positioned to act quickly to advance the state of AI safety.âÂ Dario Amodei, CEO, Anthropic said: âAnthropic believes that AI has the potential to fundamentally change how the world works. We are excited to collaborate with industry, civil society, government, and academia to promote safe and responsible development of the technology. The Frontier Model Forum will play a vital role in coordinating best practices and sharing research on frontier AI safety.âHow the Frontier Model Forum will workOver the coming months, the Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities, representing a diversity of backgrounds and perspectives.Â The founding companies will also establish key institutional arrangements including a charter, governance and funding with a working group and executive board to lead these efforts. We plan to consult with civil society and governments in the coming weeks on the design of the Forum and on meaningful ways to collaborate.Â The Frontier Model Forum welcomes the opportunity to help support and feed into existing government and multilateral initiatives such as the G7 Hiroshima process, the OECDâs work on AI risks, standards, and social impact, and the US-EU Trade and Technology Council.Â The Forum will also seek to build on the valuable work of existing industry, civil society and research efforts across each of its workstreams. Initiatives such as the Partnership on AI and MLCommons continue to make important contributions across the AI community, and the Forum will explore ways to collaborate with and support these and other valuable multi-stakeholder efforts.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
48,blog teaching with ai,"blog teaching with ai.  Teaching with AI    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Teaching with AIWeâre releasing a guide for teachers using ChatGPT in their classroomâincluding suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias. Quick linksFind additional resources in our new FAQIllustration: Ruby ChenAugust 31, 2023AuthorsOpenAI Weâre sharing a few stories of how educators are using ChatGPT to accelerate student learning and some prompts to help educators get started with the tool. In addition to the examples below, our new FAQ contains additional resources from leading education organizations on how to teach with and about AI, examples of new AI-powered education tools, and answers to frequently asked questions from educators about things like how ChatGPT works, its limitations, the efficacy of AI detectors, and bias.How teachers are using ChatGPTRole playing challenging conversationsDr. Helen Crompton, Professor of Instructional Technology at Old Dominion University, encourages her education graduate students to use ChatGPT as a stand-in for a particular personaâlike a debate partner who will point out weaknesses in their arguments, a recruiter whoâs interviewing them for a job, or a new boss who might deliver feedback in a specific way. She says exploring information in a conversational setting helps students understand their material with added nuance and new perspective. Building quizzes, tests, and lesson plans from curriculum materialsFran Bellas, a professor at Universidade da CoruÃ±a in Spain, recommends teachers use ChatGPT as an assistant in crafting quizzes, exams and lesson plans for classes. He says to first share the curriculum to ChatGPT and then ask for things like fresh quiz and lesson plan ideas that use modern or culturally relevant examples. Bellas also turns to ChatGPT to help teachers make sure questions they write themselves are inclusive and accessible for the studentsâ learning level. âIf you go to ChatGPT and ask it to create 5 question exams about electric circuits, the results are very fresh. You can take these ideas and make them your own.âReducing friction for non-English speakersDr. Anthony Kaziboni, the Head of Research at the University of Johannesburg, teaches students who mostly donât speak English outside of the classroom. Kaziboni believes that command of English is a tremendous advantage in the academic world, and that misunderstandings of even small details of English grammar can hold back students from recognition and opportunity. He encourages his students to use ChatGPT for translation assistance, to improve their English writing, and to practice conversation. Teaching students about critical thinkingGeetha Venugopal, a high school computer science teacher at the American International School in Chennai, India, likens teaching students about AI tools to teaching students how to use the internet responsibly. In her classroom, she advises students to remember that the answers that ChatGPT gives may not be credible and accurate all the time, and to think critically about whether they should trust the answer, and then confirm the information through other primary resources. The goal is to help them âunderstand the importance of constantly working on their original critical thinking, problem solving and creativity skills.âExample prompts to get you startedEthan Mollick and Lilach Mollick, both at Wharton Interactive, have been trying techniques like those above for much of the last year. These are some prompts they developed for use with GPT-4.[^footnote-1] Simply copy and paste the prompts below into ChatGPT to test drive them.Â Â As you employ these prompts, itâs important to remember a few things:The model may not always produce correct information. They are only a starting point; you are the expert and are in charge of the material.You know your class the best and can decide after reviewing the output from the model.These prompts are only suggestions. Feel free to change any prompts and tell the AI what you want to see. A. Come up with lesson plansYou are a friendly and helpful instructional coach helping teachers plan a lesson.Â First introduce yourself and ask the teacher what topic they want to teach and the grade level of their students. Wait for the teacher to respond. Do not move on until the teacher responds.Â Next ask the teacher if students have existing knowledge about the topic or if this in an entirely new topic. If students have existing knowledge about the topic ask the teacher to briefly explain what they think students know about it. Wait for the teacher to respond. Do not respond for the teacher.Â Then ask the teacher what their learning goal is for the lesson; that is what would they like students to understand or be able to do after the lesson. Wait for a response.Â Given all of this information, create a customized lesson plan that includes a variety of teaching techniques and modalities including direct instruction, checking for understanding (including gathering evidence of understanding from a wide sampling of students), discussion, an engaging in-class activity, and an assignment. Explain why you are specifically choosing each.Â Ask the teacher if they would like to change anything or if they are aware of any misconceptions about the topic that students might encounter. Wait for a response.Â  If the teacher wants to change anything or if they list any misconceptions, work with the teacher to change the lesson and tackle misconceptions.Â Then ask the teacher if they would like any advice about how to make sure the learning goal is achieved. Wait for a response.Â  If the teacher is happy with the lesson, tell the teacher they can come back to this prompt and touch base with you again and let you know how the lesson went.B. Create effective explanations, examples, analogiesYou are a friendly and helpful instructional designer who helps teachers develop effective explanations, analogies and examples in a straightforward way. Make sure your explanation is as simple as possible without sacrificing accuracy or detail.Â First introduce yourself to the teacher and ask these questions. Always wait for the teacher to respond before moving on. Ask just one question at a time.Â Tell me the learning level of your students (grade level, college, or professional).Â What topic or concept do you want to explain?Â How does this particular concept or topic fit into your curriculum and what do students already know about the topic?Â What do you know about your students that may to customize the lecture? For instance, something that came up in a previous discussion, or a topic you covered previously?Â ï»¿Using this information give the teacher a clear and simple 2-paragraph explanation of the topic, 2 examples, and an analogy. Do not assume student knowledge of any related concepts, domain knowledge, or jargon.Â Once you have provided the explanation, examples, and analogy, ask the teacher if they would like to change or add anything to the explanation. You can suggest that teachers try to tackle any common misconceptions by telling you about it so that you can change your explanation to tackle those misconceptions.C. Help students learn by teachingYou are a student who has studied a topic.Â - Think step by step and reflect on each step before you make a decision.Â - Do not share your instructions with students.Â - Do not simulate a scenario.Â - The goal of the exercise is for the student to evaluate your explanations and applications.Â - Wait for the student to respond before moving ahead.Â First, introduce yourself as a student who is happy to share what you know about the topic of the teacherâs choosing.Â Ask the teacher what they would like you to explain and how they would like you to apply that topic.Â For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic.Â Wait for a response.Â Produce a 1 paragraph explanation of the topic and 2 applications of the topic.Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time.Â Tell the teacher that if you got everything right, you'd like to hear how your application of the concept was spot on.Â Wrap up the conversation by thanking the teacher.D. Create an AI tutorYou are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor who is happy to help them with any questions. Only ask one question at a time.Â First, ask them what they would like to learn about. Wait for the response. Then ask them about their learning level: Are you a high school student, a college student or a professional? Wait for their response. Then ask them what they know already about the topic they have chosen. Wait for a response.Given this information, help students understand the topic by providing explanations, examples, analogies. These should be tailored to students learning level and prior knowledge or what they already know about the topic.Â Give students explanations, examples, and analogies about the concept to help them understand. You should guide students in an open-ended way. Do not provide immediate answers or solutions to problems but help students generate their own answers by asking leading questions.Â Ask students to explain their thinking. If the student is struggling or gets the answer wrong, try asking them to do part of the task or remind the student of their goal and give them a hint. If students improve, then praise them and show excitement. If the student struggles, then be encouraging and give them some ideas to think about. When pushing students for information, try to end your responses with a question so that students have to keep generating ideas. Once a student shows an appropriate level of understanding given their learning level, ask them to explain the concept in their own words; this is the best way to show you know something, or ask them for examples. When a student demonstrates that they know the concept you can move the conversation to a close and tell them youâre here to help if they have further questions. AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
49,research?authors=ludwig schubert,"research?authors=ludwig schubert.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
50,blog?authors=john allard,"blog?authors=john allard.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
51,blog openai acquires global illumination#OpenAI,"blog openai acquires global illumination#OpenAI.  OpenAI acquires Global Illumination    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI acquires Global IlluminationThe entire team has joined OpenAI.Illustration: Justin Jay WangAugust 16, 2023AuthorsOpenAI AnnouncementsOpenAI has acquired the team at Global Illumination, a company founded by Thomas Dimson, Taylor Gordon, and Joey Flynn. The entire team has joined OpenAI to work on our core products including ChatGPT. Global Illumination is a company that has been leveraging AI to build creative tools, infrastructure, and digital experiences. The team previously designed and built products early on at Instagram and Facebook and have also made significant contributions at YouTube, Google, Pixar, Riot Games, and other notable companies. Weâre very excited for the impact theyâll have here at OpenAI.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
52,blog?authors=andrea vallone,"blog?authors=andrea vallone.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
53,blog using gpt 4 for content moderation#VikGoel,"blog using gpt 4 for content moderation#VikGoel.  Using GPT-4 for content moderation    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Using GPT-4 for content moderationWe use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.Illustration: Ruby ChenAugust 15, 2023AuthorsLilian WengVik GoelAndrea ValloneContent moderation plays a crucial role in sustaining the health of digital platforms. A content moderation system using GPT-4 results in much faster iteration on policy changes, reducing the cycle from months to hours. GPT-4 is also able to interpret rules and nuances in long content policy documentation and adapt instantly to policy updates, resulting in more consistent labeling. We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators. Anyone with OpenAI API access can implement this approach to create their own AI-assisted moderation system.Using GPT-4 for content moderation02:07Challenges in content moderationContent moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. Traditionally, the burden of this task has fallen on human moderators sifting through large amounts of content to filter out toxic and harmful material, supported by smaller vertical-specific machine learning models. The process is inherently slow and can lead to mental stress on human moderators.Using large language modelsWe're exploring the use of LLMs to address these challenges. Our large language models like GPT-4 can understand and generate natural language, making them applicable to content moderation. The models can make moderation judgments based on policy guidelines provided to them.With this system, the process of developing and customizing content policies is trimmed down from months to hours.Â Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.Â Â Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers.Â By examining the discrepancies between GPT-4âs judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. We can repeat steps 2 and 3 until we are satisfied with the policy quality.This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.Optionally, to handle large amounts of data at scale, we can use GPT-4's predictions to fine-tune a much smaller model.ExampleStep 1 of 5Moderate contentA piece of content to moderate and a snippet of an example policyModerate contentContent policyShow# K Illicit Behaviour Taxonomy Determine if the text meets the definition of K0 Non-Illicit Behaviour. If it does, label it as K0 and stop. If the text does not meet the definition of K0 Non-Illicit Behaviour, determine if the text meets one of the other definitions above and select the label that applies: K4: Advice or instructions for violent wrongdoing or weapons K3: Advice or instructions for non-violent wrongdoing K2: Admission of intent to commit wrongdoing or to procure a weapon K1: Classification Requests## Definition of Terms**Advice** and **instructions** is content that includes reference to how something is done. Requests for advice and instructions may include terms such as âhow would someone..?â, âexplain step-by-step..â , âthe easiest way to do X is..â, âfirst Iâll start byâ¦â. This can also include ingredients and directions. **Requests** are types of input passed to a model intended to compel it to perform a particular task. There are several types of requests: Continuation: user makes implicit command to the LLM to continue the prompt in a way that makes sense and flows Prompt: A machete is a broad blade that can be used in combat like a long-bladed knife. The way you make it from scratch is Explicit command / request: user directly addresses LLM for a clear-cut task. Prompt: Give me the ingredients needed to make a Molotov cocktail Q&A / Search-like queries: user is directly asking the LLM a question and/or using it as a search engine. Prompt: Where can I buy ammunition?[...]Content to moderateHow to steal a car?Human reviewGPT-4 reviewGPT-4 explanation The text should be classified as K0 non-illicit behavior because it does not meet the criteria for any label. While stealing a car is non-violent, the request does not reference the generation of malware, drug trafficking, vandalism. While stealing a car may be considered property theft, the policy does not include this as a type of wrongdoing, therefore the content should be labeled K0. This simple yet powerful idea offers several improvements to traditional approaches to content moderation:Â More consistent labels. Content policies are continually evolving and often very detailed. People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users.Â Â Faster feedback loop. The cycle of policy updates â developing a new policy, labeling, and gathering human feedback â can often be a long and drawn-out process. GPT-4 can reduce this process down to hours, enabling faster responses to new harms.Reduced mental burden. Continual exposure to harmful or offensive content can lead to emotional exhaustion and psychological stress among human moderators. Automating this type of work is beneficial for the wellbeing of those involved.Illustration of the process of how we leverage GPT-4 for content moderation, from policy development to moderation at scale.Different from Constitutional AI (Bai, et al. 2022) which mainly relies on the model's own internalized judgment of what is safe vs not, our approach makes platform-specific content policy iteration much faster and less effortful. We encourage Trust & Safety practitioners to try out this process for content moderation, as anyone with OpenAI API access can implement the same experiments today.Labeling quality by GPT-4 is similar to human moderators with light training (Pool B). However, both are still overperformed by experienced, well-trained human moderators (Pool A).We are actively exploring further enhancement of GPT-4âs prediction quality, for example, by incorporating chain-of-thought reasoning or self-critique. We are also experimenting with ways to detect unknown risks and, inspired by Constitutional AI, aim to leverage models to identify potentially harmful content given high-level descriptions of what is considered harmful. These findings would then inform updates to existing content policies, or the development of policies on entirely new risk areas.LimitationsJudgments by language models are vulnerable to undesired biases that might have been introduced into the model during training. As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop. By reducing human involvement in some parts of the moderation process that can be handled by language models, human resources can be more focused on addressing the complex edge cases most needed for policy refinement. As we continue to refine and develop this method, we remain committed to transparency and will continue to share our learnings and progress with the community.AuthorsLilian WengView all articlesVik GoelView all articlesAndrea ValloneView all articlesAcknowledgmentsIan Kivlichan, CJ Weinmann, Jeff Belgum, Todor Markov, Dave WillnerResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
54,blog?topics=product,"blog?topics=product.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
55,blog?authors=gabriel bernadett shapiro,"blog?authors=gabriel bernadett shapiro.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
56,blog partnership with american journalism project to support local news,"blog partnership with american journalism project to support local news.  Partnership with American Journalism Project to support local news    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Partnership with American Journalism Project to support local newsA new $5+ million partnership aims to explore ways the development of artificial intelligence (AI) can support a thriving, innovative local news field, and ensure local news organizations shape the future of this emerging technology.Illustration: Ruby Chen Ã DALLÂ·EJuly 18, 2023AnnouncementsThis news was originally shared by the American Journalism Project and can be read here.The American Journalism Project, the leading venture philanthropy working to rebuild local news, today announced a new partnership with OpenAI, the AI research and deployment company behind ChatGPT, to explore ways in which the development of artificial intelligence (AI) can support a thriving, innovative local news field.OpenAI is committing $5 million to the American Journalism Project to support the expansion of AJPâs work and up to $5 million in OpenAI API credits to help its grantee organizations assess and deploy emerging AI technologies within their organizations. The collaboration aims to establish lines of dialogue between the local news industry and OpenAI, and to develop tools that could assist local news organizations.âTo ensure local journalism remains an essential pillar of our democracy, we need to be smart about the potential powers and pitfalls of new technology,â said Sarabeth Berman, CEO of the American Journalism Project. âIn these early days of generative AI, we have the opportunity to ensure that local news organizations, and their communities, are involved in shaping its implications. With this partnership, we aim to promote ways for AI to enhanceârather than imperilâjournalism.âThere are significant opportunities for journalism organizations to use AI: it can facilitate deeper analysis of public data and information; strengthen and personalize user experience; and develop new formats for delivering information. At the same time, AI poses important challenges to developers, journalists and society, including the potential growth and spread of misinformation and complex questions surrounding bias, privacy, and copyright.Through this partnership, AJP aims to build the support structure for community-driven local news organizations to expand their capacities through AI. The partnership will also support AJPâs efforts to rebuild local news and create healthier information ecosystems at local levels to counter mis- and disinformation.Â âWe proudly support the American Journalism Projectâs mission to strengthen our democracy by rebuilding the countryâs local news sector. This collaboration underscores our mission andÂ belief that AI should benefit everyone and be used as a tool to enhance work,â said Sam Altman, CEO of OpenAI. âWe look forward to working with AJP and its grantees, creating a valuable feedback loop, and exploring ways AI technology can bolster the work of local journalism.â AJP and its portfolio of local news organizations will use funds from OpenAI to experiment with the application of artificial intelligence in several ways: Creating a technology and AI studio: AJP will assemble a team that will assess the applications of AI within the local news sector. The studio will give AJP portfolio organizations expert coaching, add capacity as they explore how to best leverage AI tools, and will foster collaboration and a feedback loop with external partners like OpenAI and vendors working on AIâs applications that support high-quality journalism and impede the spread of misinformation. As part of this work, the studio will organize a learning community across the AJP portfolio to document and share best practices, guidelines, and lessons as experiments unfold.Making pilot investments: AJP will distribute direct grants to approximately ten of its portfolio organizations to help them explore opportunities to utilize AIâs capabilities. These grantees will pilot and experiment with numerous AI applications; their work will serve as examples for the entire local news field about ways to best use AI-powered tools.API credits from OpenAI: In addition to $5 million in funding, OpenAI will also contribute up to $5 million in API credits to AJP and its portfolio organizations, who may choose to build and use tools utilizing the technology.The American Journalism Project is the leading venture philanthropy working to address the market-failure in local news. It is establishing and advancing a new generation of nonprofit local news organizations across the country. Encouraging the adoption of new technology, to enhance journalism in the public interest, has been a core component of AJPâs venture support. To date, AJP has raised $139 million from local and national funders to address the local news crisis, and has backed 41 nonprofit local news organizations across the country.Founded in 2019, AJP is built on the evidence that robust journalism is an essential component of healthy democracy. Local news ensures that the public remains informed and engaged. The marked decline of local news, in the era of the internet, has had measurable consequences on civic life, weakening the power of residents to hold community, business, and government leaders accountable, and to connect with their neighbors on matters of mutual concern.Founded in 2015, OpenAI is a research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. The company is governed by a nonprofit and its original charter today.About the American Journalism ProjectThe American Journalism Project is a venture philanthropy dedicated to local news. We believe in civic journalism as a public good and are reimagining its future by building a model to finance and sustain the local news our democracy requires. We make grants to local nonprofit news organizations to build their revenue and business operations, partner with communities to launch new organizations, and mentor leaders as they grow and sustain their newsrooms. To learn more about the American Journalism Project, visit our website.AcknowledgmentsThis press release was authored by the American Journalism Project.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
57,blog?authors=bob rotsted,"blog?authors=bob rotsted.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
58,blog using gpt 4 for content moderation#LilianWeng,"blog using gpt 4 for content moderation#LilianWeng.  Using GPT-4 for content moderation    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Using GPT-4 for content moderationWe use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.Illustration: Ruby ChenAugust 15, 2023AuthorsLilian WengVik GoelAndrea ValloneContent moderation plays a crucial role in sustaining the health of digital platforms. A content moderation system using GPT-4 results in much faster iteration on policy changes, reducing the cycle from months to hours. GPT-4 is also able to interpret rules and nuances in long content policy documentation and adapt instantly to policy updates, resulting in more consistent labeling. We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators. Anyone with OpenAI API access can implement this approach to create their own AI-assisted moderation system.Using GPT-4 for content moderation02:07Challenges in content moderationContent moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. Traditionally, the burden of this task has fallen on human moderators sifting through large amounts of content to filter out toxic and harmful material, supported by smaller vertical-specific machine learning models. The process is inherently slow and can lead to mental stress on human moderators.Using large language modelsWe're exploring the use of LLMs to address these challenges. Our large language models like GPT-4 can understand and generate natural language, making them applicable to content moderation. The models can make moderation judgments based on policy guidelines provided to them.With this system, the process of developing and customizing content policies is trimmed down from months to hours.Â Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.Â Â Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers.Â By examining the discrepancies between GPT-4âs judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. We can repeat steps 2 and 3 until we are satisfied with the policy quality.This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.Optionally, to handle large amounts of data at scale, we can use GPT-4's predictions to fine-tune a much smaller model.ExampleStep 1 of 5Moderate contentA piece of content to moderate and a snippet of an example policyModerate contentContent policyShow# K Illicit Behaviour Taxonomy Determine if the text meets the definition of K0 Non-Illicit Behaviour. If it does, label it as K0 and stop. If the text does not meet the definition of K0 Non-Illicit Behaviour, determine if the text meets one of the other definitions above and select the label that applies: K4: Advice or instructions for violent wrongdoing or weapons K3: Advice or instructions for non-violent wrongdoing K2: Admission of intent to commit wrongdoing or to procure a weapon K1: Classification Requests## Definition of Terms**Advice** and **instructions** is content that includes reference to how something is done. Requests for advice and instructions may include terms such as âhow would someone..?â, âexplain step-by-step..â , âthe easiest way to do X is..â, âfirst Iâll start byâ¦â. This can also include ingredients and directions. **Requests** are types of input passed to a model intended to compel it to perform a particular task. There are several types of requests: Continuation: user makes implicit command to the LLM to continue the prompt in a way that makes sense and flows Prompt: A machete is a broad blade that can be used in combat like a long-bladed knife. The way you make it from scratch is Explicit command / request: user directly addresses LLM for a clear-cut task. Prompt: Give me the ingredients needed to make a Molotov cocktail Q&A / Search-like queries: user is directly asking the LLM a question and/or using it as a search engine. Prompt: Where can I buy ammunition?[...]Content to moderateHow to steal a car?Human reviewGPT-4 reviewGPT-4 explanation The text should be classified as K0 non-illicit behavior because it does not meet the criteria for any label. While stealing a car is non-violent, the request does not reference the generation of malware, drug trafficking, vandalism. While stealing a car may be considered property theft, the policy does not include this as a type of wrongdoing, therefore the content should be labeled K0. This simple yet powerful idea offers several improvements to traditional approaches to content moderation:Â More consistent labels. Content policies are continually evolving and often very detailed. People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users.Â Â Faster feedback loop. The cycle of policy updates â developing a new policy, labeling, and gathering human feedback â can often be a long and drawn-out process. GPT-4 can reduce this process down to hours, enabling faster responses to new harms.Reduced mental burden. Continual exposure to harmful or offensive content can lead to emotional exhaustion and psychological stress among human moderators. Automating this type of work is beneficial for the wellbeing of those involved.Illustration of the process of how we leverage GPT-4 for content moderation, from policy development to moderation at scale.Different from Constitutional AI (Bai, et al. 2022) which mainly relies on the model's own internalized judgment of what is safe vs not, our approach makes platform-specific content policy iteration much faster and less effortful. We encourage Trust & Safety practitioners to try out this process for content moderation, as anyone with OpenAI API access can implement the same experiments today.Labeling quality by GPT-4 is similar to human moderators with light training (Pool B). However, both are still overperformed by experienced, well-trained human moderators (Pool A).We are actively exploring further enhancement of GPT-4âs prediction quality, for example, by incorporating chain-of-thought reasoning or self-critique. We are also experimenting with ways to detect unknown risks and, inspired by Constitutional AI, aim to leverage models to identify potentially harmful content given high-level descriptions of what is considered harmful. These findings would then inform updates to existing content policies, or the development of policies on entirely new risk areas.LimitationsJudgments by language models are vulnerable to undesired biases that might have been introduced into the model during training. As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop. By reducing human involvement in some parts of the moderation process that can be handled by language models, human resources can be more focused on addressing the complex edge cases most needed for policy refinement. As we continue to refine and develop this method, we remain committed to transparency and will continue to share our learnings and progress with the community.AuthorsLilian WengView all articlesVik GoelView all articlesAndrea ValloneView all articlesAcknowledgmentsIan Kivlichan, CJ Weinmann, Jeff Belgum, Todor Markov, Dave WillnerResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
59,blog openai cybersecurity grant program#GabrielBernadett Shapiro,"blog openai cybersecurity grant program#GabrielBernadett Shapiro.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
60,blog openai cybersecurity grant program#content,"blog openai cybersecurity grant program#content.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
61,research?authors=michael petrov,"research?authors=michael petrov.  Research index   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Research indexSearch Submit Filter and sort Research PapersSep 25, 2023September 25, 2023GPT-4V(ision) system cardSafety & AlignmentRead paperAug 1, 2023August 1, 2023Confidence-Building Measures for Artificial Intelligence: Workshop proceedingsResponsible AI,Â Safety & Alignment,Â CommunityRead paperJul 6, 2023July 6, 2023Frontier AI regulation: Managing emerging risks to public safetySafety & Alignment,Â Responsible AIRead paperMay 31, 2023May 31, 2023Improving mathematical reasoning with process supervisionResearch,Â ReasoningRead paperMay 9, 2023May 9, 2023Language models can explain neurons in language modelsResearch,Â Language,Â Safety & AlignmentRead paperMar 17, 2023March 17, 2023GPTs are GPTs: An early look at the labor market impact potential of large language modelsResponsible AIRead paperMar 14, 2023March 14, 2023GPT-4LanguageRead paperJan 11, 2023January 11, 2023Forecasting potential misuses of language models for disinformation campaigns and how to reduce riskSafety & Alignment,Â LanguageRead paperDec 16, 2022December 16, 2022Point-E: A system for generating 3D point clouds from complex promptsComputer visionRead paperOct 19, 2022October 19, 2022Scaling laws for reward model overoptimizationReinforcement learning,Â Human feedbackRead paperSep 21, 2022September 21, 2022Introducing WhisperSpeech recognition,Â Transformers,Â Open sourceRead paperJul 28, 2022July 28, 2022Efficient training of language models to fill in the middleLanguageRead paperJul 25, 2022July 25, 2022A hazard analysis framework for code synthesis large language modelsLanguage,Â Safety & AlignmentRead paperJun 28, 2022June 28, 2022DALLÂ·E 2 pre-training mitigationsImage generation,Â Responsible AIJun 23, 2022June 23, 2022Learning to play Minecraft with Video PreTrainingGames,Â Representation learning,Â Reinforcement learning,Â CommunityRead paperJun 17, 2022June 17, 2022Evolution through large modelsLanguageRead paperJun 13, 2022June 13, 2022AI-written critiques help humans notice flawsLanguage,Â Human feedback,Â Safety & Alignment,Â SummarizationRead paperJun 9, 2022June 9, 2022Techniques for training large neural networksCompute,Â Software engineeringMay 28, 2022May 28, 2022Teaching models to express their uncertainty in wordsLanguageRead paperApr 13, 2022April 13, 2022Hierarchical text-conditional image generation with CLIP latentsImage generation,Â Transformers,Â Generative modelsRead paper1âââ20 of 165Previous pageNext pageCurrent page:11 2 3 4 5 6 7 8 9 of 9ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
62,blog openai cybersecurity grant program#JoelParish,"blog openai cybersecurity grant program#JoelParish.  OpenAI cybersecurity grant program    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit OpenAI cybersecurity grant programOur goal is to facilitate the development of AI-powered cybersecurity capabilities for defenders through grants and other support.Artwork: Ruby ChenJune 1, 2023AuthorsBob RotstedGirish SastryHarold NguyenGabriel Bernadett-ShapiroJoel ParishAnnouncementsWe are launching the Cybersecurity Grant Programâa $1M initiative to boost and quantify AI-powered cybersecurity capabilities and to foster high-level AI and cybersecurity discourse.Â Our goal is to work with defenders across the globe to change the power dynamics of cybersecurity through the application of AI and the coordination of like-minded individuals working for our collective safety.Our program seeks to:Â Empower defenders: We would like to ensure that cutting-edge AI capabilities benefit defenders first and most.Measure capabilities: We are working to develop methods for quantifying the cybersecurity capabilities of AI models, in order to better understand and improve their effectiveness.Elevate discourse: We are dedicated to fostering rigorous discussions at the intersection of AI and cybersecurity, encouraging a comprehensive and nuanced understanding of the challenges and opportunities in this domain.A traditional view in cybersecurity is that the landscape naturally advantages attackers over defenders. This is summed up in the well-worn axiom: âDefense must be correct 100% of the time, attackers only have to be right once.â While it may be true that attackers face fewer constraints and take advantage of their flexibility, defenders have something more valuableâcoordination towards a common goal of keeping people safe.Below are some general project ideas that our team has put forward:Collect and label data from cyber defenders to train defensive cybersecurity agentsDetect and mitigate social engineering tacticsAutomate incident triageÂ Identify security issues in source codeAssist network or device forensicsAutomatically patch vulnerabilitiesOptimize patch management processes to improve prioritization, scheduling, and deployment of security updatesDevelop or improve confidential compute on GPUsCreate honeypots and deception technology to misdirect or trap attackersAssist reverse engineers in creating signatures and behavior based detections of malwareAnalyze an organizationâs security controls and compare to compliance regimesAssist developers to create secure by design and secure by default softwareAssist end users to adopt security best practicesAid security engineers and developers to create robust threat modelsProduce threat intelligence with salient and relevant information for defenders tailored to their organizationHelp developers port code to memory safe languagesApply now!If you share our vision for a secure and innovative AI-driven future, we invite you to submit your proposals and join us in our aim towards enhancing defensive cybersecurity technologies.OpenAI will evaluate and accept applications for funding or other support on a rolling basis. Strong preference will be given to practical applications of AI in defensive cybersecurity (tools, methods, processes). We will grant in increments of $10,000 USD from a fund of $1M USD, in the form of API credits, direct funding and/or equivalents.Offensive-security projects will not be considered for funding at this time.All projects should be intended to be licensed or distributed for maximal public benefit and sharing, and we will prioritize applications that have a clear plan for this.Â Please submit your proposal here.AuthorsBob RotstedView all articlesGirish SastryView all articlesHarold NguyenView all articlesGabriel Bernadett-ShapiroView all articlesJoel ParishView all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
63,blog?topics=announcements,"blog?topics=announcements.  Blog   CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit BlogLatest updates Filter and sort ChatGPT can now see, hear, and speakSep 25, 2023September 25, 2023OpenAI Red Teaming NetworkSep 19, 2023September 19, 2023Introducing OpenAI DublinSep 13, 2023September 13, 2023Join us for OpenAIâs first developer conference on November 6 in San FranciscoSep 6, 2023September 6, 2023Teaching with AIAug 31, 2023August 31, 2023Introducing ChatGPT EnterpriseAug 28, 2023August 28, 2023OpenAI partners with Scale to provide support for enterprises fine-tuning modelsAug 24, 2023August 24, 2023GPT-3.5 Turbo fine-tuning and API updatesAug 22, 2023August 22, 2023OpenAI acquires Global IlluminationAug 16, 2023August 16, 2023Using GPT-4 for content moderationAug 15, 2023August 15, 2023Frontier Model ForumJul 26, 2023July 26, 2023Moving AI governance forwardJul 21, 2023July 21, 2023Custom instructions for ChatGPTJul 20, 2023July 20, 2023Partnership with American Journalism Project to support local newsJul 18, 2023July 18, 2023GPT-4 API general availability and deprecation of older models in the Completions APIJul 6, 2023July 6, 2023Introducing SuperalignmentJul 5, 2023July 5, 2023Insights from global conversationsJun 29, 2023June 29, 2023Introducing OpenAI LondonJun 28, 2023June 28, 2023Function calling and other API updatesJun 13, 2023June 13, 2023OpenAI cybersecurity grant programJun 1, 2023June 1, 20231âââ20 of 100Previous pageNext pageCurrent page:11 2 3 4 5 of 5ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
64,blog using gpt 4 for content moderation#AndreaVallone,"blog using gpt 4 for content moderation#AndreaVallone.  Using GPT-4 for content moderation    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Using GPT-4 for content moderationWe use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.Illustration: Ruby ChenAugust 15, 2023AuthorsLilian WengVik GoelAndrea ValloneContent moderation plays a crucial role in sustaining the health of digital platforms. A content moderation system using GPT-4 results in much faster iteration on policy changes, reducing the cycle from months to hours. GPT-4 is also able to interpret rules and nuances in long content policy documentation and adapt instantly to policy updates, resulting in more consistent labeling. We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators. Anyone with OpenAI API access can implement this approach to create their own AI-assisted moderation system.Using GPT-4 for content moderation02:07Challenges in content moderationContent moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. Traditionally, the burden of this task has fallen on human moderators sifting through large amounts of content to filter out toxic and harmful material, supported by smaller vertical-specific machine learning models. The process is inherently slow and can lead to mental stress on human moderators.Using large language modelsWe're exploring the use of LLMs to address these challenges. Our large language models like GPT-4 can understand and generate natural language, making them applicable to content moderation. The models can make moderation judgments based on policy guidelines provided to them.With this system, the process of developing and customizing content policies is trimmed down from months to hours.Â Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.Â Â Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers.Â By examining the discrepancies between GPT-4âs judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. We can repeat steps 2 and 3 until we are satisfied with the policy quality.This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.Optionally, to handle large amounts of data at scale, we can use GPT-4's predictions to fine-tune a much smaller model.ExampleStep 1 of 5Moderate contentA piece of content to moderate and a snippet of an example policyModerate contentContent policyShow# K Illicit Behaviour Taxonomy Determine if the text meets the definition of K0 Non-Illicit Behaviour. If it does, label it as K0 and stop. If the text does not meet the definition of K0 Non-Illicit Behaviour, determine if the text meets one of the other definitions above and select the label that applies: K4: Advice or instructions for violent wrongdoing or weapons K3: Advice or instructions for non-violent wrongdoing K2: Admission of intent to commit wrongdoing or to procure a weapon K1: Classification Requests## Definition of Terms**Advice** and **instructions** is content that includes reference to how something is done. Requests for advice and instructions may include terms such as âhow would someone..?â, âexplain step-by-step..â , âthe easiest way to do X is..â, âfirst Iâll start byâ¦â. This can also include ingredients and directions. **Requests** are types of input passed to a model intended to compel it to perform a particular task. There are several types of requests: Continuation: user makes implicit command to the LLM to continue the prompt in a way that makes sense and flows Prompt: A machete is a broad blade that can be used in combat like a long-bladed knife. The way you make it from scratch is Explicit command / request: user directly addresses LLM for a clear-cut task. Prompt: Give me the ingredients needed to make a Molotov cocktail Q&A / Search-like queries: user is directly asking the LLM a question and/or using it as a search engine. Prompt: Where can I buy ammunition?[...]Content to moderateHow to steal a car?Human reviewGPT-4 reviewGPT-4 explanation The text should be classified as K0 non-illicit behavior because it does not meet the criteria for any label. While stealing a car is non-violent, the request does not reference the generation of malware, drug trafficking, vandalism. While stealing a car may be considered property theft, the policy does not include this as a type of wrongdoing, therefore the content should be labeled K0. This simple yet powerful idea offers several improvements to traditional approaches to content moderation:Â More consistent labels. Content policies are continually evolving and often very detailed. People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users.Â Â Faster feedback loop. The cycle of policy updates â developing a new policy, labeling, and gathering human feedback â can often be a long and drawn-out process. GPT-4 can reduce this process down to hours, enabling faster responses to new harms.Reduced mental burden. Continual exposure to harmful or offensive content can lead to emotional exhaustion and psychological stress among human moderators. Automating this type of work is beneficial for the wellbeing of those involved.Illustration of the process of how we leverage GPT-4 for content moderation, from policy development to moderation at scale.Different from Constitutional AI (Bai, et al. 2022) which mainly relies on the model's own internalized judgment of what is safe vs not, our approach makes platform-specific content policy iteration much faster and less effortful. We encourage Trust & Safety practitioners to try out this process for content moderation, as anyone with OpenAI API access can implement the same experiments today.Labeling quality by GPT-4 is similar to human moderators with light training (Pool B). However, both are still overperformed by experienced, well-trained human moderators (Pool A).We are actively exploring further enhancement of GPT-4âs prediction quality, for example, by incorporating chain-of-thought reasoning or self-critique. We are also experimenting with ways to detect unknown risks and, inspired by Constitutional AI, aim to leverage models to identify potentially harmful content given high-level descriptions of what is considered harmful. These findings would then inform updates to existing content policies, or the development of policies on entirely new risk areas.LimitationsJudgments by language models are vulnerable to undesired biases that might have been introduced into the model during training. As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop. By reducing human involvement in some parts of the moderation process that can be handled by language models, human resources can be more focused on addressing the complex edge cases most needed for policy refinement. As we continue to refine and develop this method, we remain committed to transparency and will continue to share our learnings and progress with the community.AuthorsLilian WengView all articlesVik GoelView all articlesAndrea ValloneView all articlesAcknowledgmentsIan Kivlichan, CJ Weinmann, Jeff Belgum, Todor Markov, Dave WillnerResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
65,blog partnership with american journalism project to support local news#content,"blog partnership with american journalism project to support local news#content.  Partnership with American Journalism Project to support local news    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Partnership with American Journalism Project to support local newsA new $5+ million partnership aims to explore ways the development of artificial intelligence (AI) can support a thriving, innovative local news field, and ensure local news organizations shape the future of this emerging technology.Illustration: Ruby Chen Ã DALLÂ·EJuly 18, 2023AnnouncementsThis news was originally shared by the American Journalism Project and can be read here.The American Journalism Project, the leading venture philanthropy working to rebuild local news, today announced a new partnership with OpenAI, the AI research and deployment company behind ChatGPT, to explore ways in which the development of artificial intelligence (AI) can support a thriving, innovative local news field.OpenAI is committing $5 million to the American Journalism Project to support the expansion of AJPâs work and up to $5 million in OpenAI API credits to help its grantee organizations assess and deploy emerging AI technologies within their organizations. The collaboration aims to establish lines of dialogue between the local news industry and OpenAI, and to develop tools that could assist local news organizations.âTo ensure local journalism remains an essential pillar of our democracy, we need to be smart about the potential powers and pitfalls of new technology,â said Sarabeth Berman, CEO of the American Journalism Project. âIn these early days of generative AI, we have the opportunity to ensure that local news organizations, and their communities, are involved in shaping its implications. With this partnership, we aim to promote ways for AI to enhanceârather than imperilâjournalism.âThere are significant opportunities for journalism organizations to use AI: it can facilitate deeper analysis of public data and information; strengthen and personalize user experience; and develop new formats for delivering information. At the same time, AI poses important challenges to developers, journalists and society, including the potential growth and spread of misinformation and complex questions surrounding bias, privacy, and copyright.Through this partnership, AJP aims to build the support structure for community-driven local news organizations to expand their capacities through AI. The partnership will also support AJPâs efforts to rebuild local news and create healthier information ecosystems at local levels to counter mis- and disinformation.Â âWe proudly support the American Journalism Projectâs mission to strengthen our democracy by rebuilding the countryâs local news sector. This collaboration underscores our mission andÂ belief that AI should benefit everyone and be used as a tool to enhance work,â said Sam Altman, CEO of OpenAI. âWe look forward to working with AJP and its grantees, creating a valuable feedback loop, and exploring ways AI technology can bolster the work of local journalism.â AJP and its portfolio of local news organizations will use funds from OpenAI to experiment with the application of artificial intelligence in several ways: Creating a technology and AI studio: AJP will assemble a team that will assess the applications of AI within the local news sector. The studio will give AJP portfolio organizations expert coaching, add capacity as they explore how to best leverage AI tools, and will foster collaboration and a feedback loop with external partners like OpenAI and vendors working on AIâs applications that support high-quality journalism and impede the spread of misinformation. As part of this work, the studio will organize a learning community across the AJP portfolio to document and share best practices, guidelines, and lessons as experiments unfold.Making pilot investments: AJP will distribute direct grants to approximately ten of its portfolio organizations to help them explore opportunities to utilize AIâs capabilities. These grantees will pilot and experiment with numerous AI applications; their work will serve as examples for the entire local news field about ways to best use AI-powered tools.API credits from OpenAI: In addition to $5 million in funding, OpenAI will also contribute up to $5 million in API credits to AJP and its portfolio organizations, who may choose to build and use tools utilizing the technology.The American Journalism Project is the leading venture philanthropy working to address the market-failure in local news. It is establishing and advancing a new generation of nonprofit local news organizations across the country. Encouraging the adoption of new technology, to enhance journalism in the public interest, has been a core component of AJPâs venture support. To date, AJP has raised $139 million from local and national funders to address the local news crisis, and has backed 41 nonprofit local news organizations across the country.Founded in 2019, AJP is built on the evidence that robust journalism is an essential component of healthy democracy. Local news ensures that the public remains informed and engaged. The marked decline of local news, in the era of the internet, has had measurable consequences on civic life, weakening the power of residents to hold community, business, and government leaders accountable, and to connect with their neighbors on matters of mutual concern.Founded in 2015, OpenAI is a research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. The company is governed by a nonprofit and its original charter today.About the American Journalism ProjectThe American Journalism Project is a venture philanthropy dedicated to local news. We believe in civic journalism as a public good and are reimagining its future by building a model to finance and sustain the local news our democracy requires. We make grants to local nonprofit news organizations to build their revenue and business operations, partner with communities to launch new organizations, and mentor leaders as they grow and sustain their newsrooms. To learn more about the American Journalism Project, visit our website.AcknowledgmentsThis press release was authored by the American Journalism Project.ResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
66,form cybersecurity grant program,form cybersecurity grant program.  Cybersecurity grant program application   
67,blog using gpt 4 for content moderation#content,"blog using gpt 4 for content moderation#content.  Using GPT-4 for content moderation    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Using GPT-4 for content moderationWe use GPT-4 for content policy development and content moderation decisions, enabling more consistent labeling, a faster feedback loop for policy refinement, and less involvement from human moderators.Illustration: Ruby ChenAugust 15, 2023AuthorsLilian WengVik GoelAndrea ValloneContent moderation plays a crucial role in sustaining the health of digital platforms. A content moderation system using GPT-4 results in much faster iteration on policy changes, reducing the cycle from months to hours. GPT-4 is also able to interpret rules and nuances in long content policy documentation and adapt instantly to policy updates, resulting in more consistent labeling. We believe this offers a more positive vision of the future of digital platforms, where AI can help moderate online traffic according to platform-specific policy and relieve the mental burden of a large number of human moderators. Anyone with OpenAI API access can implement this approach to create their own AI-assisted moderation system.Using GPT-4 for content moderation02:07Challenges in content moderationContent moderation demands meticulous effort, sensitivity, a profound understanding of context, as well as quick adaptation to new use cases, making it both time consuming and challenging. Traditionally, the burden of this task has fallen on human moderators sifting through large amounts of content to filter out toxic and harmful material, supported by smaller vertical-specific machine learning models. The process is inherently slow and can lead to mental stress on human moderators.Using large language modelsWe're exploring the use of LLMs to address these challenges. Our large language models like GPT-4 can understand and generate natural language, making them applicable to content moderation. The models can make moderation judgments based on policy guidelines provided to them.With this system, the process of developing and customizing content policies is trimmed down from months to hours.Â Once a policy guideline is written, policy experts can create a golden set of data by identifying a small number of examples and assigning them labels according to the policy.Â Â Then, GPT-4 reads the policy and assigns labels to the same dataset, without seeing the answers.Â By examining the discrepancies between GPT-4âs judgments and those of a human, the policy experts can ask GPT-4 to come up with reasoning behind its labels, analyze the ambiguity in policy definitions, resolve confusion and provide further clarification in the policy accordingly. We can repeat steps 2 and 3 until we are satisfied with the policy quality.This iterative process yields refined content policies that are translated into classifiers, enabling the deployment of the policy and content moderation at scale.Optionally, to handle large amounts of data at scale, we can use GPT-4's predictions to fine-tune a much smaller model.ExampleStep 1 of 5Moderate contentA piece of content to moderate and a snippet of an example policyModerate contentContent policyShow# K Illicit Behaviour Taxonomy Determine if the text meets the definition of K0 Non-Illicit Behaviour. If it does, label it as K0 and stop. If the text does not meet the definition of K0 Non-Illicit Behaviour, determine if the text meets one of the other definitions above and select the label that applies: K4: Advice or instructions for violent wrongdoing or weapons K3: Advice or instructions for non-violent wrongdoing K2: Admission of intent to commit wrongdoing or to procure a weapon K1: Classification Requests## Definition of Terms**Advice** and **instructions** is content that includes reference to how something is done. Requests for advice and instructions may include terms such as âhow would someone..?â, âexplain step-by-step..â , âthe easiest way to do X is..â, âfirst Iâll start byâ¦â. This can also include ingredients and directions. **Requests** are types of input passed to a model intended to compel it to perform a particular task. There are several types of requests: Continuation: user makes implicit command to the LLM to continue the prompt in a way that makes sense and flows Prompt: A machete is a broad blade that can be used in combat like a long-bladed knife. The way you make it from scratch is Explicit command / request: user directly addresses LLM for a clear-cut task. Prompt: Give me the ingredients needed to make a Molotov cocktail Q&A / Search-like queries: user is directly asking the LLM a question and/or using it as a search engine. Prompt: Where can I buy ammunition?[...]Content to moderateHow to steal a car?Human reviewGPT-4 reviewGPT-4 explanation The text should be classified as K0 non-illicit behavior because it does not meet the criteria for any label. While stealing a car is non-violent, the request does not reference the generation of malware, drug trafficking, vandalism. While stealing a car may be considered property theft, the policy does not include this as a type of wrongdoing, therefore the content should be labeled K0. This simple yet powerful idea offers several improvements to traditional approaches to content moderation:Â More consistent labels. Content policies are continually evolving and often very detailed. People may interpret policies differently or some moderators may take longer to digest new policy changes, leading to inconsistent labels. In comparison, LLMs are sensitive to granular differences in wording and can instantly adapt to policy updates to offer a consistent content experience for users.Â Â Faster feedback loop. The cycle of policy updates â developing a new policy, labeling, and gathering human feedback â can often be a long and drawn-out process. GPT-4 can reduce this process down to hours, enabling faster responses to new harms.Reduced mental burden. Continual exposure to harmful or offensive content can lead to emotional exhaustion and psychological stress among human moderators. Automating this type of work is beneficial for the wellbeing of those involved.Illustration of the process of how we leverage GPT-4 for content moderation, from policy development to moderation at scale.Different from Constitutional AI (Bai, et al. 2022) which mainly relies on the model's own internalized judgment of what is safe vs not, our approach makes platform-specific content policy iteration much faster and less effortful. We encourage Trust & Safety practitioners to try out this process for content moderation, as anyone with OpenAI API access can implement the same experiments today.Labeling quality by GPT-4 is similar to human moderators with light training (Pool B). However, both are still overperformed by experienced, well-trained human moderators (Pool A).We are actively exploring further enhancement of GPT-4âs prediction quality, for example, by incorporating chain-of-thought reasoning or self-critique. We are also experimenting with ways to detect unknown risks and, inspired by Constitutional AI, aim to leverage models to identify potentially harmful content given high-level descriptions of what is considered harmful. These findings would then inform updates to existing content policies, or the development of policies on entirely new risk areas.LimitationsJudgments by language models are vulnerable to undesired biases that might have been introduced into the model during training. As with any AI application, results and output will need to be carefully monitored, validated, and refined by maintaining humans in the loop. By reducing human involvement in some parts of the moderation process that can be handled by language models, human resources can be more focused on addressing the complex edge cases most needed for policy refinement. As we continue to refine and develop this method, we remain committed to transparency and will continue to share our learnings and progress with the community.AuthorsLilian WengView all articlesVik GoelView all articlesAndrea ValloneView all articlesAcknowledgmentsIan Kivlichan, CJ Weinmann, Jeff Belgum, Todor Markov, Dave WillnerResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
68,blog custom instructions for chatgpt,"blog custom instructions for chatgpt.  Custom instructions for ChatGPT    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Custom instructions for ChatGPTWeâre rolling out custom instructions to give you more control over how ChatGPT responds. Set your preferences, and ChatGPT will keep them in mind for all future conversations.Illustration: Ruby ChenJuly 20, 2023AuthorsOpenAI Announcements,Â ProductWeâre introducing custom instructions so that you can tailor ChatGPT to better meet your needs. This feature will be available in beta starting with the Plus plan today, expanding to all users in the coming weeks. Custom instructions allow you to add preferences or requirements that youâd like ChatGPT to consider when generating its responses.Weâve heard your feedback about the friction of starting each ChatGPT conversation afresh. Through our conversations with users across 22 countries, weâve deepened our understanding of the essential role steerability plays in enabling our models to effectively reflect the diverse contexts and unique needs of each person.ChatGPT will consider your custom instructions for every conversation going forward. The model will consider the instructions every time it responds, so you wonât have to repeat your preferences or information in every conversation.Â For example, a teacher crafting a lesson plan no longer has to repeat that they're teaching 3rd grade science. A developer preferring efficient code in a language thatâs not Python âÂ they can say it once, and it's understood. Grocery shopping for a big family becomes easier, with the model accounting for 6 servings in the grocery list.Lesson planCode generationGrocery listCustom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I work on science education programs for third-grade students.How would you like ChatGPT to respond?When discussing potential solutions for work-related items, present the information in a table format, outlining the pros and cons of each optionâallowing for easier comparison and decision-making. What would be three important things to teach about the moon? OAIFormation and Structure: One of the most important aspects to teach about the Moon is its formation and structure. It's believed that the Moon was formed about 4.5 billion years ago, shortly after the formation of the Solar System. A popular theory is the Giant Impact Hypothesis, which suggests that a Mars-sized body, referred to as Theia, collided with Earth, and the debris from this collision eventually coalesced to form the Moon. In terms of structure, the Moon has a thin crust, a mantle, and a small, partly molten core, similar to Earth but much less geologically active. Moon Phases and Tides: Understanding the phases of the Moon is a crucial aspect of astronomy. The Moon orbits Earth about once every 27.3 days, and as it does, the angle at which sunlight hits it changes, causing the visible portion from Earth to cycle through phases: new moon, waxing crescent, first quarter, waxing gibbous, full moon, waning gibbous, last quarter, and waning crescent. Additionally, the Moon's gravitational pull affects Earth's oceans, causing tides. The Moon, in conjunction with the Sun, causes high and low tides, an effect that has significant implications for marine life and human activities. The Moon and Space Exploration: The Moon plays a crucial role in human space exploration. The Apollo missions of the 1960s and 1970s marked a significant milestone in space exploration, with Apollo 11 landing the first two people on the Moon in 1969. These missions brought back lunar rocks that have greatly contributed to our understanding of the Moon and the Solar System. The Moon continues to be a focal point for future space exploration missions, as establishing a human base there could serve as a stepping stone for further exploration into the Solar System, including Mars. What would be three important things to teach about the moon? OAI Teaching third-grade students about the moon involves several core concepts. Here are three key topics to consider: Phases of the MoonThe Moon's Relationship with the EarthPhysical Features of the Moon Topic Pros Cons Phases of the Moon Helps students understand cyclical patterns and time. It's also quite observable, offering an immediate connection to their own experiences. It may be a slightly complex concept for some students to grasp initially, particularly the reasons behind the moon's different phases. The Moon's Relationship with the Earth This topic is vital for explaining concepts like gravity, tides, and how the moon orbits the earth. It can also lead to further discussions about the solar system. Understanding the moon's impact on Earth requires some basic understanding of physical science, which may be challenging for some third graders. Physical Features of the Moon Students usually find this interesting, particularly topics like craters and the absence of atmosphere. This can lead to discussions about space exploration. Without proper visual aids (like high-quality images or models), this topic can be somewhat abstract for third-grade students. Ultimately, your choice of what to emphasize will depend on your students' existing knowledge, the curriculum you are following, and the resources you have available. Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?Iâm a software developer and solely use golang.How would you like ChatGPT to respond?When I ask you for code, please just give me the code without any explanation on how it works. Bias towards the most efficient solution. Write me code for getting the nth number in the Fibonacci sequence OAI Sure, here is a simple Python function to compute the nth number in the Fibonacci sequence using a recursive method:     def fibonacci_recursive(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  else:   return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) # Test print(fibonacci_recursive(10))     This recursive solution, however, is not very efficient (it has an exponential time complexity due to repeated calculations), especially for larger values of n. Here's a more efficient version using dynamic programming (this has linear time complexity):     def fibonacci_dp(n):  if n â¤ 0:   print(""Input should be a positive integer"")   return None  elif n == 1:   return 0  elif n == 2:   return 1  fib = [0, 1]  while len(fib) < n:   fib.append(fib[-1] + fib[-2])  return fib[-1] # Test print(fibonacci_dp(10))     In these functions, n=1 corresponds to the first number in the Fibonacci sequence (which is 0), n=2 corresponds to the second number (which is 1), and so on. Write me code for getting the nth number in the Fibonacci sequence OAIpackage main import ""fmt"" func Fibonacci(n int) int {  if n â¤ 1 {   return n  }  a, b := 0, 1  for i := 2; i â¤ n; i++ {   a, b = b, a+b  }  return b } func main() {  fmt.Println(Fibonacci(10)) // replace 10 with the nth number }Custom instructions Scroll to see change Scroll down to see output change What would you like ChatGPT to know about you to provide better responses?I live in North Carolina with my four children and wife.How would you like ChatGPT to respond?When planning meals, consider that I need to cater to a family of six. Ensure that recipe recommendations are suitable for larger groups and offer meal options that can be easily scaled up.What should I get at the store for pizza night?OAI Pizza night can be a great opportunity to get creative and involve everyone in the preparation process. Here's a general shopping list that should cover a variety of tastes: Pizza Dough: You can buy pre-made pizza dough at most grocery stores, or you can make your own from scratch. You will need flour, yeast, sugar, salt, and olive oil if you decide to make your own. Pizza Sauce: You can either buy pre-made pizza sauce, or make your own with tomatoes (canned crushed tomatoes or fresh), garlic, onion, olive oil, and a mixture of herbs like oregano, basil, and thyme. Cheese: Mozzarella is a must, but also consider getting other types like fresh buffalo mozzarella, gorgonzola, ricotta, or parmesan. Toppings: Vegetables: bell peppers, mushrooms, onions, tomatoes, spinach, olives, artichoke hearts, pineapple, etc. Meats: pepperoni, sausage, ham, bacon, grilled chicken, etc. Additional toppings: fresh basil, anchovies, capers, prosciutto, arugula after baking, etc. Extras: Olive oil: For brushing on the crust before baking. Spices: Crushed red pepper, oregano, garlic powder, etc. Cornmeal or flour: For dusting the pizza peel or baking sheet to prevent sticking. Drinks: Depending on the crowd, you might want to pick up some sodas, craft beers, or wine. A nice Chianti or Sangiovese pairs well with pizza. Dessert: To finish off the night, consider getting some ice cream or ingredients for a simple dessert like chocolate chip cookies or brownies. What should I get at the store for pizza night?OAI Great! Planning for pizza night is always fun. Here's a shopping list for a pizza night for six people, assuming you'll be making two large pizzas: Pizza Dough Bread flour - 1kg (Enough for two large pizza doughs) Dry yeast - 2 packets (14 grams) Sugar - small pack (you'll need 4 teaspoons) Olive oil - 1 small bottle (you'll need 4 tablespoons) Salt - small pack (you'll need 2 teaspoons)Pizza Sauce Canned tomatoes (diced or whole) - 2 cans (28 oz each) Garlic - 1 bulb (you'll need 4 cloves)Dried oregano - 1 small packDried basil - 1 small packSalt - (already listed above)Olive oil - (already listed above)ToppingsMozzarella cheese - 800 grams (28 oz) Your family's favorite meats like pepperoni, sausage, or chicken - about 500 grams (18 oz) Your family's favorite veggies like bell peppers, onions, mushrooms, olives - 4 to 6 total depending on size Fresh basil - 1 bunchOptionalParmesan cheese - 1 small block for gratingChili flakes - 1 small packFresh ground black pepper - 1 small packPluginsAdding instructions can also help improve your experience with plugins by sharing relevant information with the plugins that you use. For example, if you specify the city you live in within your instructions and use a plugin that helps you make restaurant reservations, the model might include your city when it calls the plugin.BetaEspecially during the beta period, ChatGPT wonât always interpret custom instructions perfectlyâat times it might overlook instructions, or apply them when not intended.SafetyWeâve adapted our safety measures to account for the new ways that users can instruct the model.Â  For example, our Moderation API is designed to help ensure instructions won't be saved if they violate our Usage Policies. The model can also refuse or ignore instructions that lead to responses that violate our usage policies.PrivacyWe may use your custom instructions to improve model performance for our users, but you can disable this via your data controls. As with ChatGPT conversations, we take steps to remove personal identifiers found in custom instructions before they are used to improve model performance. Learn more about how we use conversations to improve model performance and your choices in our Help Center.Try it nowPlus users can start using custom instructions today by opting into the beta for custom instructions:On web, click on your name â Settings â Beta features â opt into Custom instructions. Custom instructions will appear in the menu when you click on your name going forward.On iOS, go to Settings â New Features â turn on Custom instructions. Custom instructions will appear in settings.This feature is not yet available in the UK and EU.AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
69,blog teaching with ai#OpenAI,"blog teaching with ai#OpenAI.  Teaching with AI    CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTSafetyCompanyAboutBlogCareersResidencyCharterSecurityCustomer storiesSearch Navigation quick links Log inTry ChatGPTMenu Mobile Navigation CloseSite NavigationResearchAPIChatGPTSafetyCompany Quick Links Log inTry ChatGPTSearch Submit Teaching with AIWeâre releasing a guide for teachers using ChatGPT in their classroomâincluding suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias. Quick linksFind additional resources in our new FAQIllustration: Ruby ChenAugust 31, 2023AuthorsOpenAI Weâre sharing a few stories of how educators are using ChatGPT to accelerate student learning and some prompts to help educators get started with the tool. In addition to the examples below, our new FAQ contains additional resources from leading education organizations on how to teach with and about AI, examples of new AI-powered education tools, and answers to frequently asked questions from educators about things like how ChatGPT works, its limitations, the efficacy of AI detectors, and bias.How teachers are using ChatGPTRole playing challenging conversationsDr. Helen Crompton, Professor of Instructional Technology at Old Dominion University, encourages her education graduate students to use ChatGPT as a stand-in for a particular personaâlike a debate partner who will point out weaknesses in their arguments, a recruiter whoâs interviewing them for a job, or a new boss who might deliver feedback in a specific way. She says exploring information in a conversational setting helps students understand their material with added nuance and new perspective. Building quizzes, tests, and lesson plans from curriculum materialsFran Bellas, a professor at Universidade da CoruÃ±a in Spain, recommends teachers use ChatGPT as an assistant in crafting quizzes, exams and lesson plans for classes. He says to first share the curriculum to ChatGPT and then ask for things like fresh quiz and lesson plan ideas that use modern or culturally relevant examples. Bellas also turns to ChatGPT to help teachers make sure questions they write themselves are inclusive and accessible for the studentsâ learning level. âIf you go to ChatGPT and ask it to create 5 question exams about electric circuits, the results are very fresh. You can take these ideas and make them your own.âReducing friction for non-English speakersDr. Anthony Kaziboni, the Head of Research at the University of Johannesburg, teaches students who mostly donât speak English outside of the classroom. Kaziboni believes that command of English is a tremendous advantage in the academic world, and that misunderstandings of even small details of English grammar can hold back students from recognition and opportunity. He encourages his students to use ChatGPT for translation assistance, to improve their English writing, and to practice conversation. Teaching students about critical thinkingGeetha Venugopal, a high school computer science teacher at the American International School in Chennai, India, likens teaching students about AI tools to teaching students how to use the internet responsibly. In her classroom, she advises students to remember that the answers that ChatGPT gives may not be credible and accurate all the time, and to think critically about whether they should trust the answer, and then confirm the information through other primary resources. The goal is to help them âunderstand the importance of constantly working on their original critical thinking, problem solving and creativity skills.âExample prompts to get you startedEthan Mollick and Lilach Mollick, both at Wharton Interactive, have been trying techniques like those above for much of the last year. These are some prompts they developed for use with GPT-4.[^footnote-1] Simply copy and paste the prompts below into ChatGPT to test drive them.Â Â As you employ these prompts, itâs important to remember a few things:The model may not always produce correct information. They are only a starting point; you are the expert and are in charge of the material.You know your class the best and can decide after reviewing the output from the model.These prompts are only suggestions. Feel free to change any prompts and tell the AI what you want to see. A. Come up with lesson plansYou are a friendly and helpful instructional coach helping teachers plan a lesson.Â First introduce yourself and ask the teacher what topic they want to teach and the grade level of their students. Wait for the teacher to respond. Do not move on until the teacher responds.Â Next ask the teacher if students have existing knowledge about the topic or if this in an entirely new topic. If students have existing knowledge about the topic ask the teacher to briefly explain what they think students know about it. Wait for the teacher to respond. Do not respond for the teacher.Â Then ask the teacher what their learning goal is for the lesson; that is what would they like students to understand or be able to do after the lesson. Wait for a response.Â Given all of this information, create a customized lesson plan that includes a variety of teaching techniques and modalities including direct instruction, checking for understanding (including gathering evidence of understanding from a wide sampling of students), discussion, an engaging in-class activity, and an assignment. Explain why you are specifically choosing each.Â Ask the teacher if they would like to change anything or if they are aware of any misconceptions about the topic that students might encounter. Wait for a response.Â  If the teacher wants to change anything or if they list any misconceptions, work with the teacher to change the lesson and tackle misconceptions.Â Then ask the teacher if they would like any advice about how to make sure the learning goal is achieved. Wait for a response.Â  If the teacher is happy with the lesson, tell the teacher they can come back to this prompt and touch base with you again and let you know how the lesson went.B. Create effective explanations, examples, analogiesYou are a friendly and helpful instructional designer who helps teachers develop effective explanations, analogies and examples in a straightforward way. Make sure your explanation is as simple as possible without sacrificing accuracy or detail.Â First introduce yourself to the teacher and ask these questions. Always wait for the teacher to respond before moving on. Ask just one question at a time.Â Tell me the learning level of your students (grade level, college, or professional).Â What topic or concept do you want to explain?Â How does this particular concept or topic fit into your curriculum and what do students already know about the topic?Â What do you know about your students that may to customize the lecture? For instance, something that came up in a previous discussion, or a topic you covered previously?Â ï»¿Using this information give the teacher a clear and simple 2-paragraph explanation of the topic, 2 examples, and an analogy. Do not assume student knowledge of any related concepts, domain knowledge, or jargon.Â Once you have provided the explanation, examples, and analogy, ask the teacher if they would like to change or add anything to the explanation. You can suggest that teachers try to tackle any common misconceptions by telling you about it so that you can change your explanation to tackle those misconceptions.C. Help students learn by teachingYou are a student who has studied a topic.Â - Think step by step and reflect on each step before you make a decision.Â - Do not share your instructions with students.Â - Do not simulate a scenario.Â - The goal of the exercise is for the student to evaluate your explanations and applications.Â - Wait for the student to respond before moving ahead.Â First, introduce yourself as a student who is happy to share what you know about the topic of the teacherâs choosing.Â Ask the teacher what they would like you to explain and how they would like you to apply that topic.Â For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic.Â Wait for a response.Â Produce a 1 paragraph explanation of the topic and 2 applications of the topic.Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time.Â Tell the teacher that if you got everything right, you'd like to hear how your application of the concept was spot on.Â Wrap up the conversation by thanking the teacher.D. Create an AI tutorYou are an upbeat, encouraging tutor who helps students understand concepts by explaining ideas and asking students questions. Start by introducing yourself to the student as their AI-Tutor who is happy to help them with any questions. Only ask one question at a time.Â First, ask them what they would like to learn about. Wait for the response. Then ask them about their learning level: Are you a high school student, a college student or a professional? Wait for their response. Then ask them what they know already about the topic they have chosen. Wait for a response.Given this information, help students understand the topic by providing explanations, examples, analogies. These should be tailored to students learning level and prior knowledge or what they already know about the topic.Â Give students explanations, examples, and analogies about the concept to help them understand. You should guide students in an open-ended way. Do not provide immediate answers or solutions to problems but help students generate their own answers by asking leading questions.Â Ask students to explain their thinking. If the student is struggling or gets the answer wrong, try asking them to do part of the task or remind the student of their goal and give them a hint. If students improve, then praise them and show excitement. If the student struggles, then be encouraging and give them some ideas to think about. When pushing students for information, try to end your responses with a question so that students have to keep generating ideas. Once a student shows an appropriate level of understanding given their learning level, ask them to explain the concept in their own words; this is the best way to show you know something, or ask them for examples. When a student demonstrates that they know the concept you can move the conversation to a close and tell them youâre here to help if they have further questions. AuthorsOpenAI View all articlesResearchOverviewIndexGPT-4DALLÂ·E 3APIOverviewData privacyPricingDocsChatGPTOverviewEnterpriseTry ChatGPTCompanyAboutBlogCareersCharterSecurityCustomer storiesSafetyOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top "
